{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"87bc140c","cell_type":"markdown","source":"# Google Gen AI Capstone Project Q1:\n\nThis notebook consolidates three projects from the **Google Gen AI Capstone Project Q1**:\n\n### ðŸ“ Project 1: AI Noise Reduction\n- Uses Gemini to clean noisy sentences and restore clarity using prompt engineering and text generation.\n\n### ðŸ“ Project 2: Anomaly Detection in Text\n- Leverages Gemini's embedding API to detect outliers in textual datasets by comparing distances in vector space.\n\n### ðŸ“ Project 3: Convert Notebook to PDF\n- Demonstrates a simple Gradio UI to upload a notebook and convert it into a styled PDF or HTML output.\n\nEach project retains its original structure and content.","metadata":{}},{"id":"612c2943","cell_type":"markdown","source":"## Conversational Audio Restoration Agent using Gemini and Gradio\n\nThis notebook implements an interactive web application where users can upload audio files, ask questions about them (like \"What is the noise floor?\"), and request noise reduction (like \"Remove noise by 5 dB\"). The application uses the Google Gemini API for natural language understanding and function calling, `librosa` and `noisereduce` for audio processing, and `Gradio` for the user interface.\n\n**Key Features:**\n\n*   **Conversational Interface:** Interact with the system using natural language queries.\n*   **Audio Upload:** Supports uploading audio files (WAV recommended).\n*   **Noise Floor Analysis:** Estimates and reports the background noise level in dB using the `get_noise_floor` function.\n*   **Noise Reduction:** Applies noise reduction using spectral gating via the `reduce_noise_by_db` function, controlled by the user's request (e.g., \"by 5 dB\").\n*   **Gemini Function Calling:** Leverages Gemini's ability to understand the user's intent and automatically call the appropriate Python function (`get_noise_floor` or `reduce_noise_by_db`) with the correct parameters.\n*   **Visual Feedback:** Displays spectrograms of the original and denoised audio for visual comparison.\n*   **Audio Playback:** Allows playback of the original (via re-upload if needed) and denoised audio.\n\n**Potential Future Enhancements:**\n\n*   **Support for More Formats:** Improve audio loading robustness (e.g., ensure FFmpeg is reliably used) to handle MP3, M4A, etc.\n*   **Advanced Noise Reduction:** Integrate more sophisticated denoising models (e.g., deep learning based) or offer different algorithm choices.\n*   **Parameter Tuning:** Allow users to fine-tune noise reduction parameters via the UI (e.g., aggressiveness, frequency range).\n*   **Streaming Audio:** Support processing real-time audio streams.\n*   **Deployment:** Package the application for deployment (e.g., using Docker, Hugging Face Spaces).\n","metadata":{}},{"id":"ef320330","cell_type":"code","source":"# Remove unused conflicting packages\n#!pip uninstall -qqy jupyterlab kfp 2>/dev/null\n# Install specific google-genai version used in the original notebook\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"eef2f4bd","cell_type":"markdown","source":"## 1. Install Dependencies\n\n*   This cell installs the necessary Python libraries required for the application.\n*   `google-genai`: The official Google Gemini SDK for Python.\n*   `gradio`: Used to create the interactive web UI.\n*   `librosa`: A powerful library for audio analysis (loading, spectrograms).\n*   `matplotlib`: Used by librosa for plotting spectrograms.\n*   `noisereduce`: Performs the noise reduction algorithm.\n*   `soundfile`: Used by librosa (and directly) for reading/writing audio files (often needs system libraries like `libsndfile`).\n*   `numpy`: Fundamental package for numerical operations.\n*   `ffmpeg-python`: Python bindings for FFmpeg. `librosa`'s fallback audio loading mechanism (`audioread`) often requires the FFmpeg multimedia framework to be installed on your system to handle various audio formats (like MP3 or certain WAV encodings). You might need to install FFmpeg separately using your system's package manager (e.g., `sudo apt update && sudo apt install ffmpeg` on Debian/Ubuntu, `brew install ffmpeg` on macOS).\n","metadata":{}},{"id":"df45f5ba","cell_type":"code","source":"# Install required packages if not already installed\n# Uncomment the line below to run the installation if needed\n!pip install  gradio librosa matplotlib noisereduce soundfile numpy ffmpeg-python --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3ed1bf7a","cell_type":"markdown","source":"## 2. Import Libraries\n\n*   Imports all the necessary modules from the installed libraries for use in the script.\n*   `os`: For interacting with the operating system (e.g., setting environment variables, path operations).\n*   `gradio` as `gr`: For building the user interface.\n*   `numpy` as `np`: For numerical calculations.\n*   `librosa`, `librosa.display`: For audio loading and spectrogram visualization.\n*   `matplotlib.pyplot` as `plt`: For finalizing and customizing plots.\n*   `noisereduce` as `nr`: For the noise reduction function.\n*   `soundfile` as `sf`: For writing audio files.\n*   `google.genai` as `genai`, `google.genai.types`: For interacting with the Gemini API and its specific types.\n*   `traceback`: For getting detailed error information in exception handlers.\n*   `time`: For generating unique timestamps for filenames.","metadata":{}},{"id":"20e52c81","cell_type":"code","source":"import os\nimport gradio as gr\nimport numpy as np\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport noisereduce as nr\nimport soundfile as sf\nfrom google import genai\nfrom google.genai import types\nimport traceback # Import traceback for better error details\nimport time","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"id":"20d8b308","cell_type":"markdown","source":"## 3. Initialize Gemini Client\n\n*   Sets the Google API key from an environment variable. **Remember to replace `\"YOUR_GEMINI_API_KEY\"` with your actual key.**\n*   Creates the Gemini API client instance (`genai.Client`).\n","metadata":{}},{"id":"b1103900","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nprint(\"Running on Kaggle, API key loaded from Kaggle Secrets.\")\nclient = genai.Client(api_key=api_key)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"id":"4d239af2","cell_type":"markdown","source":"## 4. Define Audio Analysis Tool Functions\n\nThese Python functions will be made available to the Gemini model. The Gemini SDK's function calling feature allows the model to decide when to execute these functions based on the user's query.\n\n#### `get_noise_floor(audio_path: str) -> dict`\n\n*   **Purpose:** Estimates the background noise level (noise floor) of an audio file.\n*   **Input:** `audio_path` (string) - The absolute path to the audio file.\n*   **Process:**\n    1.  Checks if the provided `audio_path` actually exists. Returns an error dictionary if not.\n    2.  Loads the audio file using `librosa.load(audio_path, sr=None)`. `sr=None` preserves the original sample rate. Librosa might use `soundfile` or fallback to `audioread` (which may need FFmpeg).\n    3.  Handles cases where the audio file is empty or contains only silence.\n    4.  Estimates the noise floor amplitude: It calculates the 10th percentile of the *absolute* values of the audio samples. This is a simple heuristic assuming that the quietest 10% of the signal largely represents background noise.\n    5.  Converts the noise amplitude to decibels (dB) relative to the maximum possible amplitude (0 dBFS). The formula used is $$ \\text{dB} = 10 \\log_{10}(\\text{amplitude}^2 + 10^{-10}) $$. A small value ($$10^{-10}$$) is added before the logarithm for numerical stability, preventing $$ \\log_{10}(0) $$.\n*   **Output:** A dictionary containing either the calculated noise floor (`{'noise_floor_db': float_value}`) or an error message (`{'noise_floor_db': None, 'error': '...'}`).\n*   **Gemini Integration:** The function's docstring and type hints allow the Gemini SDK to automatically create a schema. Gemini will call this function when the user asks a question like \"What is the noise floor?\".\n\n#### `reduce_noise_by_db(audio_path: str, reduction_db: int) -> dict`\n\n*   **Purpose:** Reduces background noise in an audio file using a spectral gating algorithm provided by the `noisereduce` library.\n*   **Inputs:**\n    *   `audio_path` (string): The absolute path to the input audio file.\n    *   `reduction_db` (integer): The desired amount of noise reduction in decibels (dB). A higher value means more aggressive reduction.\n*   **Process:**\n    1.  Checks if the input `audio_path` exists.\n    2.  Loads the audio file using `librosa.load`.\n    3.  Handles empty audio files.\n    4.  Estimates the noise profile: It takes the first 0.5 seconds of the audio (`noise_clip`) as representative of the background noise. Handles cases where the audio is shorter than 0.5 seconds or if the noise clip contains non-finite values (e.g., `NaN`, `inf`).\n    5.  Applies noise reduction using `noisereduce.reduce_noise`:\n        *   `y`: The full audio signal array.\n        *   `sr`: The audio sample rate.\n        *   `y_noise`: The noise profile estimated from `noise_clip`.\n        *   `prop_decrease`: Controls the aggressiveness of the noise reduction. The `noisereduce` library expects a proportion (0 to 1), so the input `reduction_db` is scaled (approximately `reduction_db / 20`) and clamped between 0 and 1.\n    6.  Generates a unique output filename using a timestamp (`denoised_{timestamp}.wav`) and constructs its absolute path in the current working directory.\n    7.  Saves the noise-reduced audio (`reduced`) to the new file using `soundfile.write`.\n    8.  Verifies that the output file was created successfully and is not empty using `os.path.exists` and `os.path.getsize`.\n*   **Output:** A dictionary containing either the absolute path to the denoised file (`{'denoised_path': '/path/to/denoised_....wav'}`) or an error message (`{'denoised_path': None, 'error': '...'}`).\n*   **Gemini Integration:** Gemini calls this function when the user asks to \"remove noise\", \"denoise\", etc., inferring the `reduction_db` amount from the query.\n","metadata":{}},{"id":"5333b968","cell_type":"code","source":"# --- Audio Analysis Functions (for Gemini Function Calling) ---\n\ndef get_noise_floor(audio_path: str) -> dict:\n    \"\"\"\n    Calculate the noise floor (in dB) of the given audio file.\n    The noise floor is estimated based on the 10th percentile amplitude.\n\n    Args:\n        audio_path: Absolute path to the audio file (wav recommended).\n\n    Returns:\n        Dictionary with 'noise_floor_db' as float, or 'error' on failure.\n    \"\"\"\n    try:\n        # Check if the file exists before trying to load\n        if not os.path.exists(audio_path):\n            print(f\"[get_noise_floor] Error: File not found at specified path: {audio_path}\")\n            return {\"noise_floor_db\": None, \"error\": f\"File not found: {audio_path}\"}\n\n        # Load audio using librosa (sr=None preserves original sample rate)\n        y, sr = librosa.load(audio_path, sr=None)\n\n        # Handle empty or silent audio\n        if y.size == 0 or np.all(y == 0):\n            print(f\"[get_noise_floor] Warning: Audio file is empty or silent: {audio_path}\")\n            return {\"noise_floor_db\": -np.inf} # Represent silence as negative infinity dB\n\n        # Estimate noise floor amplitude (10th percentile of absolute signal)\n        noise_amplitude = np.percentile(np.abs(y), 10)\n\n        # Convert amplitude to dB (relative to 1.0)\n        # Add epsilon (1e-10) for numerical stability to avoid log10(0)\n        noise_floor_db = 10 * np.log10(noise_amplitude**2 + 1e-10)\n\n        # print(f\"[get_noise_floor] Calculated noise floor for {os.path.basename(audio_path)}: {noise_floor_db:.2f} dB\") # Optional debug log\n        return {\"noise_floor_db\": float(noise_floor_db)}\n    except Exception as e:\n        # Log and return error if any step fails\n        print(f\"[get_noise_floor] ERROR processing {os.path.basename(audio_path)}: {e}\")\n        return {\"noise_floor_db\": None, \"error\": str(e)}\n\ndef reduce_noise_by_db(audio_path: str, reduction_db: int) -> dict:\n    \"\"\"\n    Reduce noise in the audio file by a specified dB amount using spectral gating.\n\n    Args:\n        audio_path: Absolute path to the input audio file (wav recommended).\n        reduction_db: Amount of noise reduction desired in dB (e.g., 5, 10).\n\n    Returns:\n        Dictionary with 'denoised_path' (absolute path to output wav file), or 'error'.\n    \"\"\"\n    try:\n        # Check if input file exists\n        if not os.path.exists(audio_path):\n            print(f\"[reduce_noise_by_db] Error: Input file not found: {audio_path}\")\n            return {\"denoised_path\": None, \"error\": f\"Input file not found: {audio_path}\"}\n\n        # Load audio\n        y, sr = librosa.load(audio_path, sr=None)\n\n        # Handle empty audio\n        if y.size == 0:\n            print(f\"[reduce_noise_by_db] Warning: Input audio is empty: {audio_path}\")\n            return {\"denoised_path\": None, \"error\": \"Input audio is empty\"}\n\n        # Estimate noise profile from the beginning of the audio (e.g., first 0.5 seconds)\n        noise_clip_len = min(len(y), int(sr*0.5)) # Use min to handle short audio\n        if noise_clip_len == 0:\n             print(f\"[reduce_noise_by_db] Warning: Audio too short for noise profiling: {audio_path}\")\n             # If audio is extremely short, maybe just return original or error\n             return {\"denoised_path\": audio_path, \"error\": \"Audio too short for noise profiling\"}\n        noise_clip = y[:noise_clip_len]\n\n        # Ensure noise profile contains valid numbers\n        if not np.all(np.isfinite(noise_clip)):\n             print(f\"[reduce_noise_by_db] Error: Non-finite values detected in noise clip for {audio_path}\")\n             return {\"denoised_path\": None, \"error\": \"Non-finite values detected in noise clip\"}\n\n        # Perform noise reduction using noisereduce library\n        # `prop_decrease` controls reduction amount (0-1 scale), map from dB and clamp\n        prop_decrease = min(max(reduction_db / 20.0, 0.0), 1.0) # Clamp between 0 and 1\n        print(f\"[reduce_noise_by_db] Applying noise reduction with prop_decrease={prop_decrease:.2f} (from {reduction_db} dB)\")\n        reduced_audio = nr.reduce_noise(y=y, sr=sr, y_noise=noise_clip, prop_decrease=prop_decrease)\n\n        # Generate unique output filename and absolute path in the current directory\n        timestamp = int(time.time())\n        out_filename = f\"denoised_{timestamp}.wav\"\n        out_path = os.path.join(os.getcwd(), out_filename) # Use absolute path\n\n        # Save the denoised audio\n        sf.write(out_path, reduced_audio, sr)\n\n        # Verify file was saved successfully\n        if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n            print(f\"[reduce_noise_by_db] Noise reduction successful. Saved to: {out_path} (Size: {os.path.getsize(out_path)} bytes)\")\n            return {\"denoised_path\": out_path} # Return the absolute path\n        else:\n            # Handle file saving errors\n            error_msg = f\"Failed to write or created empty denoised file: {out_path}\"\n            print(f\"[reduce_noise_by_db] ERROR: {error_msg}\")\n            return {\"denoised_path\": None, \"error\": error_msg}\n    except Exception as e:\n        # Log and return error if any step fails\n        print(f\"[reduce_noise_by_db] ERROR processing {os.path.basename(audio_path)}: {e}\")\n        # traceback.print_exc() # Uncomment for full traceback during debugging\n        return {\"denoised_path\": None, \"error\": str(e)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e845d7c1","cell_type":"markdown","source":"## 5. Define Spectrogram Plotting Function\n\n*   **`plot_spectrogram(audio_path, title)`**:\n    *   Takes an audio file path and a title string.\n    *   Checks if the path is valid and the file exists and is not empty.\n    *   Loads the audio using `librosa.load`.\n    *   Calculates the Short-Time Fourier Transform (STFT) using `librosa.stft`.\n    *   Converts the STFT magnitude to decibels using `librosa.amplitude_to_db`.\n    *   Uses `librosa.display.specshow` to create a spectrogram plot (frequency vs. time, color intensity represents dB level).\n    *   Adds a title and color bar.\n    *   Uses `plt.close(fig)` to prevent the plot from displaying directly in the notebook output (Gradio will handle displaying it).\n    *   Returns the `matplotlib.figure.Figure` object for Gradio to display, or `None` if an error occurred.\n","metadata":{}},{"id":"b3cafc6d","cell_type":"code","source":"# --- Spectrogram Plotting Function ---\n\ndef plot_spectrogram(audio_path, title=\"Spectrogram\"):\n    \"\"\"\n    Generates a spectrogram plot for the given audio file.\n\n    Args:\n        audio_path: Absolute path to the audio file.\n        title: Title for the plot.\n\n    Returns:\n        A matplotlib Figure object containing the plot, or None on error.\n    \"\"\"\n    try:\n        # Validate input path and file existence/size\n        if not audio_path or not isinstance(audio_path, str) or not os.path.exists(audio_path):\n             print(f\"[plot_spectrogram] Skipped: File not found or path invalid: {audio_path}\")\n             return None\n        # Check file size to avoid errors with empty files\n        if os.path.getsize(audio_path) == 0:\n            print(f\"[plot_spectrogram] Skipped: Audio file is empty: {audio_path}\")\n            return None\n\n        # print(f\"[plot_spectrogram] Plotting: {audio_path}\") # Optional debug log\n        # Load audio\n        y, sr = librosa.load(audio_path, sr=None)\n\n        # Handle empty loaded audio data (should be caught by size check, but belt-and-suspenders)\n        if y.size == 0:\n            print(f\"[plot_spectrogram] Skipped: Loaded audio data is empty after load: {audio_path}\")\n            return None\n\n        # Compute STFT and convert to dB scale (log magnitude)\n        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n\n        # Create the plot using matplotlib\n        fig, ax = plt.subplots(figsize=(8, 3)) # Adjust figsize as needed\n        # Display spectrogram with log frequency axis\n        img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax)\n        ax.set(title=title) # Set plot title\n        fig.colorbar(img, ax=ax, format=\"%+2.0f dB\") # Add color bar showing dB scale\n        plt.tight_layout() # Adjust layout\n\n        # IMPORTANT: Close the plot figure object to prevent duplicate display\n        plt.close(fig)\n\n        # print(f\"[plot_spectrogram] Success for: {audio_path}\") # Optional debug log\n        return fig # Return the figure object for Gradio\n    except Exception as e:\n        # Log errors during plotting\n        print(f\"[plot_spectrogram] ERROR plotting spectrogram for {os.path.basename(audio_path)}: {e}\")\n        # traceback.print_exc() # Uncomment for detailed traceback during debugging\n        return None # Return None if plotting fails\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"id":"c93c627f","cell_type":"markdown","source":"## 6. Define Main Agent Logic\n\n*   **`agent(audio, user_query)`**: This is the core function that Gradio calls when the user clicks \"Submit\".\n    *   **Inputs:**\n        *   `audio`: The audio data uploaded by the user (as a tuple `(sample_rate, numpy_array)` because `type=\"numpy\"` is used in `gr.Audio`).\n        *   `user_query`: The text entered by the user.\n    *   **Process:**\n        1.  Performs initial checks: verifies the Gemini client is initialized and the audio input is valid.\n        2.  Generates a unique input filename and saves the uploaded audio data to an absolute path using `soundfile.write`. Checks if saving was successful.\n        3.  Defines the list of available Python tool functions (`tools = [get_noise_floor, reduce_noise_by_db]`).\n        4.  Defines the `system_instruction` to guide the Gemini model on how to behave and use the tools.\n        5.  Constructs the prompt for Gemini, explicitly including the absolute path to the saved input audio file.\n        6.  Prepares the `GenerateContentConfig` object, passing the `tools` list and the `system_instruction`.\n        7.  Calls the Gemini API using `client.models.generate_content`, providing the model name, the user prompt (`contents`), and the `config`. This triggers the automatic function calling process if needed.\n        8.  Initializes variables for the output text, denoised audio path, and spectrogram plots. Plots the original spectrogram immediately using the absolute input path.\n        9.  **Parses Function Call Results from History:** Iterates through the `response.automatic_function_calling_history`. This history contains the sequence of model turns and tool executions. It looks specifically for parts with a `function_response` added back by the SDK (usually under the `user` role in the history).\n        10. If a `function_response` is found, it extracts the nested `result` dictionary (which contains the actual dictionary returned by your Python function, e.g., `{'denoised_path': '...'}`).\n        11. Based on the function name (`func_name`), it updates the `output_text` by appending the function result, assigns the `denoised_audio_path` (using the absolute path returned by the function), and calls `plot_spectrogram` for the denoised file if applicable. Includes checks for valid results (e.g., path exists).\n        12. Retrieves the final text generated by the model (after function calls) from `response.candidates`.\n        13. Includes robust error handling using a `try...except` block around the entire process.\n    *   **Outputs:** Returns a tuple containing the values needed to update the Gradio output components: `(output_text, orig_spec_plot, denoised_spec_plot, denoised_audio_path)`.\n","metadata":{}},{"id":"e8d26b5a","cell_type":"code","source":"# --- Gradio + Gemini Conversational Agent Logic ---\n\ndef agent(audio, user_query):\n    \"\"\"\n    Handles user interaction via Gradio: saves audio, calls Gemini with tools,\n    parses results, generates plots, and returns outputs for the UI.\n    \"\"\"\n    # --- 1. Input Validation & Setup ---\n    if client is None:\n        # Check if Gemini client failed to initialize\n        return \"ERROR: Gemini client not initialized. Please check API Key and restart.\", None, None, None\n    if audio is None or not isinstance(audio, tuple) or len(audio) != 2:\n        return \"ERROR: Invalid audio input. Please upload a valid audio file.\", None, None, None\n\n    sample_rate, audio_data = audio\n    if audio_data is None or audio_data.size == 0:\n        return \"ERROR: Audio data is empty.\", None, None, None\n\n    # Generate unique absolute path for the input file\n    timestamp = int(time.time())\n    input_audio_filename = f\"input_{timestamp}.wav\"\n    input_audio_path = os.path.join(os.getcwd(), input_audio_filename) # Use absolute path\n\n    # Initialize output variables\n    output_text = \"Processing...\"\n    orig_spec_plot = None\n    denoised_spec_plot = None\n    denoised_audio_path = None # Store path to the final denoised audio\n\n    try:\n        # --- 2. Save Uploaded Audio ---\n        print(f\"[agent] Saving uploaded audio to: {input_audio_path}\")\n        sf.write(input_audio_path, audio_data, sample_rate)\n        # Verify save operation\n        if not os.path.exists(input_audio_path) or os.path.getsize(input_audio_path) == 0:\n             print(f\"[agent] ERROR: Failed to save uploaded audio file to {input_audio_path}\")\n             return \"[agent] Error: Failed to save uploaded audio file.\", None, None, None\n        print(f\"[agent] Successfully saved input file.\")\n\n        # --- 3. Prepare Gemini API Call ---\n        tools = [get_noise_floor, reduce_noise_by_db]\n        system_instruction_text = (\n            \"You are an audio analysis assistant. Always use the provided tools to answer questions about noise floor or to denoise audio. \"\n            \"Do not attempt to answer directlyâ€”always invoke the relevant function. The audio file path is specified in the user prompt.\"\n        )\n        config = types.GenerateContentConfig(\n            tools=tools,\n            system_instruction=system_instruction_text\n        )\n        # Pass the absolute path to Gemini in the prompt\n        prompt_for_gemini = f\"{user_query}\\n\\nAudio path: '{input_audio_path}'\"\n        contents = [{\"role\": \"user\", \"parts\": [{\"text\": prompt_for_gemini}]}]\n\n        # --- 4. Call Gemini API ---\n        print(f\"[agent] Sending request to Gemini for file: {input_audio_path}\")\n        response = client.models.generate_content(\n            model=\"gemini-1.5-flash-latest\", # Use a model supporting function calling\n            contents=contents,\n            config=config\n        )\n        print(f\"[agent] Received response from Gemini.\")\n\n        # --- 5. Process Gemini Response ---\n        # Get the final text response generated by the model (after function calls)\n        final_text_response = \"\"\n        if response.candidates and response.candidates[0].content.parts:\n             final_text_response = \"\".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))\n        output_text = final_text_response if final_text_response else \"Processing complete.\"\n\n        # Plot original spectrogram immediately (using absolute path)\n        orig_spec_plot = plot_spectrogram(input_audio_path, \"Original Spectrogram\")\n\n        # Parse function call results from the automatic history\n        if hasattr(response, 'automatic_function_calling_history'):\n            print(f\"[agent] Parsing automatic_function_calling_history...\")\n            history = response.automatic_function_calling_history\n            # Iterate through history to find the SDK-added FunctionResponse\n            for content in reversed(history): # Look from end\n                 if content.role == 'user' and content.parts: # SDK adds result as 'user' role\n                     for part in content.parts:\n                         if hasattr(part, \"function_response\") and part.function_response is not None:\n                            func_name = part.function_response.name\n                            func_response_data = part.function_response.response # Outer dict: {'result': {...}}\n                            print(f\"[agent]   Found FunctionResponse in history for: {func_name}\")\n\n                            # Check the expected nested structure: {'result': {actual_dict}}\n                            if isinstance(func_response_data, dict) and 'result' in func_response_data:\n                                actual_result = func_response_data.get('result')\n\n                                if isinstance(actual_result, dict):\n                                    # --- Handle get_noise_floor result ---\n                                    if func_name == \"get_noise_floor\":\n                                        if 'noise_floor_db' in actual_result and actual_result.get('noise_floor_db') is not None and np.isfinite(actual_result.get('noise_floor_db', np.nan)):\n                                            noise_db = actual_result['noise_floor_db']\n                                            output_text += f\"\\n\\n[Function Result]: Noise floor is {noise_db:.2f} dB\"\n                                        else:\n                                             error_msg = actual_result.get('error', 'invalid value')\n                                             output_text += f\"\\n\\n[Function Error]: Could not get noise floor - {error_msg}\"\n\n                                    # --- Handle reduce_noise_by_db result ---\n                                    elif func_name == \"reduce_noise_by_db\":\n                                        if 'denoised_path' in actual_result:\n                                            path_value = actual_result.get('denoised_path')\n                                            # Check if path is valid (non-None string) AND file exists\n                                            if path_value and isinstance(path_value, str) and os.path.exists(path_value):\n                                                denoised_audio_path = path_value # Assign the absolute path\n                                                print(f\"[agent]     Assigned denoised path: {denoised_audio_path}\")\n                                                if \"Noise reduction successful\" not in output_text: # Append if not already said by model\n                                                     output_text += f\"\\n\\n[Function Result]: Noise reduction successful. Output path: {os.path.basename(denoised_audio_path)}\"\n                                                # Plot the denoised spectrogram\n                                                denoised_spec_plot = plot_spectrogram(denoised_audio_path, \"Denoised Spectrogram\")\n                                            else:\n                                                 error_msg = f\"Path '{path_value}' from function invalid or file missing.\"\n                                                 print(f\"[agent]     ERROR: {error_msg}\")\n                                                 if \"Noise reduction failed\" not in output_text: output_text += f\"\\n\\n[Function Error]: Noise reduction failed - {error_msg}\"\n                                        else:\n                                             error_msg = actual_result.get('error', \"'denoised_path' key missing\")\n                                             print(f\"[agent]     ERROR: {error_msg}\")\n                                             if \"Noise reduction failed\" not in output_text: output_text += f\"\\n\\n[Function Error]: Noise reduction failed - {error_msg}\"\n                                    # If only one function call expected, can break history loop here\n                                    # break\n                                # else: print error if actual_result wasn't a dict\n                                # else: print error if 'result' key missing or not dict\n        else:\n             print(\"[agent] No automatic_function_calling_history found or parsed.\")\n\n        # --- 6. Final Output Preparation ---\n        print(\"-\" * 20)\n        print(\"[agent] Values FINALIZED for Gradio:\")\n        print(f\"[agent] Output Text: {output_text[:500]}...\")\n        print(f\"[agent] Original Plot Type: {type(orig_spec_plot)}\")\n        print(f\"[agent] Denoised Plot Type: {type(denoised_spec_plot)}\")\n        print(f\"[agent] Denoised Audio Path: {denoised_audio_path}\")\n        print(\"-\" * 20)\n\n        # Return values in the order expected by Gradio outputs\n        return output_text, orig_spec_plot, denoised_spec_plot, denoised_audio_path\n\n    # --- 7. Error Handling ---\n    except Exception as e:\n        # Catch any unexpected errors during the agent execution\n        error_details = traceback.format_exc()\n        print(f\"[agent] FATAL ERROR in agent function: {error_details}\")\n        error_message = f\"An unexpected error occurred: {str(e)}\"\n        # Return error message to the user\n        return f\"ERROR: {error_message}\\n\\n(Details logged server-side)\", None, None, None\n    # --- 8. Cleanup (Optional) ---\n    finally:\n        # Example: Delete the temporary input file after processing\n        try:\n            if input_audio_path and os.path.exists(input_audio_path):\n                # os.remove(input_audio_path)\n                # print(f\"[agent] Cleaned up input file: {input_audio_path}\")\n                pass # Keeping files for now for inspection\n        except Exception as clean_e:\n            print(f\"[agent] Error during cleanup: {clean_e}\")\n        # Avoid deleting denoised_audio_path here as Gradio needs it\n        pass\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"id":"f4a0e26b","cell_type":"markdown","source":"## 7. Define Gradio User Interface\n\n*   Uses `gr.Blocks` for a custom UI layout.\n*   `gr.Markdown`: Displays introductory text.\n*   `gr.Row`, `gr.Column`: Organizes components horizontally and vertically.\n*   `gr.Audio`:\n    *   Input (`audio_input`): Allows users to upload or record audio. `type=\"numpy\"` makes the callback function (`agent`) receive the audio as a tuple `(sample_rate, numpy_array)`.\n    *   Output (`denoised_audio`): Displays the processed audio file. `type=\"filepath\"` means it expects an absolute file path string from the `agent` function. `interactive=False` prevents user input on this output component.\n*   `gr.Textbox`:\n    *   Input (`user_query`): For the user's text request.\n    *   Output (`output_text`): Displays the text response from the agent. `interactive=False`.\n*   `gr.Plot`: Displays the spectrogram images (`orig_spec`, `denoised_spec`) generated by `plot_spectrogram`.\n*   `gr.Button`: The \"Submit\" button.\n*   `.click()`: Connects the button click event to the `agent` function, mapping UI inputs (`audio_input`, `user_query`) to the function's arguments and the function's return values to the UI outputs (`output_text`, `orig_spec`, `denoised_spec`, `denoised_audio`) in the specified order.\n","metadata":{}},{"id":"7b933918","cell_type":"code","source":"# --- Gradio User Interface Definition ---\n\n# Use gr.Blocks for more layout control\nwith gr.Blocks() as demo:\n    # Add a title and description using Markdown\n    gr.Markdown(\"# Conversational Audio Restoration Agent ðŸŽ¤\")\n    gr.Markdown(\"Upload audio, ask questions (e.g., 'What is the noise floor?', 'Remove noise by 5 dB'), and see the results.\")\n\n    # Arrange components in rows and columns\n    with gr.Row():\n        # Left column for user inputs\n        with gr.Column(scale=1):\n            # Component for audio upload/recording\n            audio_input = gr.Audio(\n                label=\"Upload Audio (WAV recommended)\",\n                type=\"numpy\" # Provides (sample_rate, data_array) to the backend function\n            )\n            # Textbox for the user's natural language query\n            user_query = gr.Textbox(\n                label=\"Ask the agent\",\n                placeholder=\"e.g. 'What is the noise floor?', 'Remove noise by 5 dB'\"\n            )\n            # Button to trigger the agent function\n            btn = gr.Button(\"Submit\", variant=\"primary\") # 'primary' makes it stand out\n\n        # Right column for displaying outputs\n        with gr.Column(scale=2):\n            # Textbox to show the agent's text response\n            output_text = gr.Textbox(\n                label=\"Agent Response\",\n                lines=5, # Allow multiple lines for longer responses\n                interactive=False # Output only\n            )\n            # Row specifically for the two plots side-by-side\n            with gr.Row():\n                 # Placeholder for the original audio spectrogram plot\n                 orig_spec = gr.Plot(label=\"Original Spectrogram\")\n                 # Placeholder for the denoised audio spectrogram plot\n                 denoised_spec = gr.Plot(label=\"Denoised Spectrogram\")\n            # Component to play back the denoised audio file\n            denoised_audio = gr.Audio(\n                label=\"Denoised Audio Output\",\n                type=\"filepath\", # Expects a file path from the backend function\n                interactive=False # Output only\n            )\n\n    # Define the action when the button is clicked\n    btn.click(\n        fn=agent,                           # The Python function to execute\n        inputs=[audio_input, user_query],   # Components providing input to the function\n        outputs=[output_text, orig_spec, denoised_spec, denoised_audio] # Components to update with function's return values\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0afb8c89","cell_type":"markdown","source":"## 8. Launch Gradio App\n\n*   Checks if the `client` object was successfully initialized in Step 3.\n*   If the client is ready, it calls `demo.launch(debug=True)`.\n    *   `demo.launch()` starts the Gradio web server, making the UI accessible via a local URL (or a public one if `share=True` is used, though be cautious with API keys).\n    *   `debug=True` provides more detailed error messages directly in the browser console and server logs if something goes wrong within Gradio or the callback function, which is very helpful during development.\n*   If the client initialization failed, it prints an error message instead of launching the app.\n","metadata":{}},{"id":"0012e9eb","cell_type":"code","source":"# --- Launch Gradio App ---\n\n# Check if the Gemini client was initialized successfully before launching the web UI\nif __name__ == \"__main__\":\n    # This check prevents running the server if the API key is invalid, for example\n    if client:\n        print(\"Gemini client initialized. Launching Gradio app...\")\n        # Start the Gradio web server interface\n        # debug=True provides helpful error messages during development\n        # share=True can create a temporary public link (use with caution regarding API keys/data)\n        demo.launch(debug=True)\n    else:\n        # Inform the user if the app cannot launch due to client initialization failure\n        print(\"ERROR: Gradio app cannot launch because Gemini client failed to initialize. Please check API key and restart.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"52a1ef6c-0bc0-42f2-8123-4d3fc3b0eb4b","cell_type":"code","source":"demo.close()\nprint(\"Gradio app closed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3582f20c-38d8-4df7-86d9-74caef055d5e","cell_type":"markdown","source":"---\n---","metadata":{}},{"id":"012039c3","cell_type":"markdown","source":"# Anomaly Detection with Gemini API\n\nThis notebook demonstrates how to detect anomalies in text data using **embeddings** generated by the Gemini API.\n\n## ðŸ’¡ Capabilities of this Notebook\n- Run in **Kaggle or Google Colab** seamlessly\n- Loads and preprocesses newsgroup text data from multiple domains\n- Generates text embeddings using Google's Gemini API\n- Defines a subset of text (**science newsgroups**) as 'normal' data\n- Introduces anomalies by **mixing in unrelated categories in test data**\n- Calculates semantic distances using embeddings\n- **Computes/Finds anomaly text in test data**\n\n\n## ðŸ”§ Possible Enhancements\n- Try different distance metrics (cosine, Mahalanobis)\n- Use alternative embedding models (e.g., BERT, OpenAI)\n- Introduce real-world noisy or domain-shifted data\n- Save and reuse embeddings to avoid repeated API calls","metadata":{}},{"id":"13ea73c5","cell_type":"code","source":"# Remove unused conflicting packages\n#!pip uninstall -qqy jupyterlab kfp 2>/dev/null\n# Install specific google-genai version used in the original notebook\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c9588b6b","cell_type":"markdown","source":"## ðŸ“š Import Necessary Libraries\n\nWe import the required Python modules for:\n- Data loading and preprocessing\n- Using the Gemini API to embed text\n- Distance computation for anomaly scoring\n","metadata":{}},{"id":"409f8ab5","cell_type":"code","source":"# Load newsgroup dataset with selected categories\nfrom google import genai\nfrom google.genai import types\nfrom google.api_core import retry\nimport google.api_core.exceptions # Often needed for specific error types\n\nprint(f\"Using google-genai version: {genai.__version__}\")\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf # Still needed for random seed setting initially\nimport time\nimport concurrent.futures\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\nimport email # Standard Python library for parsing email messages\nimport re # Regular expressions for pattern matching and text cleaning\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics.pairwise import cosine_distances # For distance calculation\nfrom sklearn.utils import Bunch # To help manage data sourcing workaround\n\n\n# For demonstration purposes, we use a random seed for reproducibility.\nnp.random.seed(42)\ntf.random.set_seed(42) # Keep for numpy seed consistency if needed elsewhere\n\n# Define the retry predicate function (checks for rate limit or server errors)\n# Ensure this runs before functions using the @retry decorator\nis_retriable = lambda e: isinstance(e, (\n    genai.errors.APIError, # General API errors (includes 503)\n    google.api_core.exceptions.ResourceExhausted, # Specific error for 429\n    google.api_core.exceptions.DeadlineExceeded,\n    google.api_core.exceptions.ServiceUnavailable # Handles 503\n)) and (not hasattr(e, 'code') or e.code in {429, 503}) # Check code if available\n\nprint(\"Imports and retry logic set up.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a845bdad","cell_type":"markdown","source":"## ðŸ” Set Up Gemini API Key\n\nTo use the Gemini API for generating embeddings, an API key is required.\n\n### Options:\n- **Kaggle**: Secrets are stored under the Kaggle environment and fetched automatically.\n- **Colab**: Use `getpass` or manually input your key. Alternatively, you can use `os.environ`.\n\n> ðŸ’¡ Make sure your API key has access to the embedding endpoint.","metadata":{}},{"id":"3b321fbb","cell_type":"code","source":"# Authenticate and create embedding model client\nimport os\n# Make sure genai is imported if not done earlier in Cell 3\n# import google.generativeai as genai\n\n# --- Auto-detect Environment and Get API Key ---\nclient = None # Initialize client to None\nGOOGLE_API_KEY = None\nenvironment = \"unknown\"\n\nprint(\"Attempting to detect environment and configure Google GenAI client...\")\n\n# --- Environment Detection using Environment Variables ---\nif 'COLAB_GPU' in os.environ:\n    print(\"Detected Colab environment via COLAB_GPU.\")\n    environment = \"colab\"\nelif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    print(\"Detected Kaggle environment via KAGGLE_KERNEL_RUN_TYPE.\")\n    environment = \"kaggle\"\nelse:\n    # Fallback check using imports if variables aren't definitive\n    try:\n        from google.colab import userdata\n        print(\"Detected Colab environment via import.\")\n        environment = \"colab\"\n    except ImportError:\n        try:\n            from kaggle_secrets import UserSecretsClient\n            print(\"Detected Kaggle environment via import.\")\n            environment = \"kaggle\"\n        except ImportError:\n             print(\"Could not detect Colab or Kaggle environment via variables or imports.\")\n             environment = \"other\"\n\n\n# --- Get API Key based on detected environment ---\nif environment == \"colab\":\n    try:\n        from google.colab import userdata # Import again just in case\n        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n        print(\"Successfully retrieved GOOGLE_API_KEY from Colab secrets.\")\n    except userdata.SecretNotFoundError:\n        print(\"Secret 'GOOGLE_API_KEY' not found in Colab secrets.\")\n    except Exception as e:\n        print(f\"An error occurred retrieving Colab secret: {type(e).__name__}: {e}\")\n\nelif environment == \"kaggle\":\n    try:\n        from kaggle_secrets import UserSecretsClient # Import again just in case\n        GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n        print(\"Successfully retrieved GOOGLE_API_KEY from Kaggle secrets.\")\n    except Exception as e: # Catch potential errors during secret retrieval\n         print(f\"An error occurred retrieving Kaggle secret: {type(e).__name__}: {e}\")\n         # Check if it's specifically the secret not found error if possible\n         if \"Secret not found\" in str(e): # Simple string check\n              print(\"Secret 'GOOGLE_API_KEY' not found in Kaggle secrets.\")\n         else:\n              print(\"Please ensure the secret 'GOOGLE_API_KEY' is added to this notebook.\")\n\nelif environment == \"other\":\n     # Try environment variable as a last resort\n     print(\"Trying OS environment variable 'GOOGLE_API_KEY'.\")\n     GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n     if GOOGLE_API_KEY:\n          print(\"Found GOOGLE_API_KEY in OS environment variables.\")\n     else:\n          print(\"GOOGLE_API_KEY not found as OS environment variable.\")\nelse: # Should not happen, but handle unknown case\n     print(\"Environment detection resulted in an unexpected state.\")\n\n\n# --- Initialize Client ---\nif GOOGLE_API_KEY:\n    try:\n        # Ensure genai.Client is available\n        if hasattr(genai, 'Client'):\n             client = genai.Client(api_key=GOOGLE_API_KEY)\n             print(\"Successfully configured Google GenAI client.\")\n             # Optional: Test client\n             # try:\n             #      client.models.list()\n             #      print(\"Client connection test successful.\")\n             # except Exception as test_e:\n             #      print(f\"Client connection test failed: {test_e}\")\n             #      client = None # Reset client if test fails\n        else:\n             print(\"Error: genai.Client class not found. Was the library imported correctly?\")\n             client = None\n\n    except Exception as client_e:\n        print(f\"\\n--- ERROR: Failed to configure client ---\")\n        print(f\"An error occurred during client configuration: {type(client_e).__name__}: {client_e}\")\n        client = None\nelse:\n    print(\"\\n--- WARNING ---\")\n    print(\"GOOGLE_API_KEY could not be retrieved.\")\n    print(\"GenAI Client could not be configured.\")\n    print(\"API calls to Gemini will FAIL.\")\n    print(\"--- END WARNING ---\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c4765740","cell_type":"markdown","source":"## Dataset\n\n* The [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) is used as the source for our 'normal' data.\n* We load the raw data, preprocess it, and then sample subsets from 'sci.*' categories to define our 'normal' data group.","metadata":{}},{"id":"dbf42fb3","cell_type":"code","source":"# Load newsgroup dataset with selected categories\n# Load the raw data needed later for finding anomalies\n# Keep these variables accessible\nprint(\"Loading initial train/test splits...\")\n# Use try-except to handle potential network errors during fetch\ntry:\n    newsgroups_train_raw = fetch_20newsgroups(subset=\"train\")\n    newsgroups_test_raw = fetch_20newsgroups(subset=\"test\")\n    print(f\"Raw train posts: {len(newsgroups_train_raw.data)}\")\n    print(f\"Raw test posts: {len(newsgroups_test_raw.data)}\")\n    print(f\"All categories: {newsgroups_train_raw.target_names}\")\nexcept Exception as e:\n    print(f\"ERROR loading dataset: {e}\")\n    print(\"Please ensure internet is enabled for the notebook and try again.\")\n    # Optionally raise the error to stop execution if data loading is critical\n    raise e","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"00fc79dc","cell_type":"markdown","source":"## Prepare the Dataset\n\n### **Objective**\n\n* **Preprocessing**: Clean the raw newsgroup posts.\n* **Normalization**: Format the text to resemble standard prose.\n\n### **Processing Steps**\n\n* **Extract**: Use email headers (e.g., \"Subject\") and the message payload.\n* **Remove**: Eliminate email addresses and common headers/footers.\n* **Truncate**: Limit the text length (e.g., 5000 characters).","metadata":{}},{"id":"b518fd6d","cell_type":"code","source":"def preprocess_newsgroup_row(data):\n    \"\"\"\n    Processes a single email/newsgroup entry:\n    - Extracts the subject and body.\n    - Removes email addresses and common clutter.\n    - Truncates text to 5,000 characters.\n\n    Args:\n        data (str): Raw email message as a string.\n\n    Returns:\n        str: Cleaned and truncated text.\n    \"\"\"\n    # Parse the email message from the raw string format\n    try:\n        msg = email.message_from_string(data)\n        # Extract subject and body text\n        subject = msg['Subject'] if msg['Subject'] else \"\"\n        payload = msg.get_payload() if msg.get_payload() else \"\"\n        # Ensure payload is a string\n        if isinstance(payload, list): # Handle multipart messages simply\n            # Decode parts if necessary, assuming utf-8 or latin-1\n            payload_parts = []\n            for part in payload:\n                try:\n                    if hasattr(part, 'get_payload'):\n                        p_content = part.get_payload(decode=True)\n                        if p_content:\n                             try:\n                                  payload_parts.append(p_content.decode('utf-8'))\n                             except UnicodeDecodeError:\n                                  try:\n                                       payload_parts.append(p_content.decode('latin-1'))\n                                  except UnicodeDecodeError:\n                                       payload_parts.append(\"[Undecodable Content]\") # Placeholder\n                        else:\n                            payload_parts.append(str(part)) # Fallback for non-payload parts\n                    else:\n                         payload_parts.append(str(part)) # Fallback for non-message parts\n                except Exception as part_e:\n                     payload_parts.append(f\"[Error processing part: {part_e}]\")\n            payload = \"\\n\".join(payload_parts)\n\n        elif isinstance(payload, bytes):\n             try:\n                   payload = payload.decode('utf-8')\n             except UnicodeDecodeError:\n                   try:\n                        payload = payload.decode('latin-1')\n                   except UnicodeDecodeError:\n                        payload = \"[Undecodable Bytes Payload]\"\n\n\n        text = f\"{subject}\\n\\n{str(payload)}\" # Ensure payload is string\n\n    except Exception as e:\n        # Handle potential parsing errors on malformed messages\n        # print(f\"Warning: Error parsing email data: {e}. Using raw data snippet.\")\n        text = str(data)[:5000] # Use raw data as fallback\n\n    # Remove email addresses from the text\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Remove common header/footer lines (simple examples)\n    text = re.sub(r'^\\s*Lines: \\d+\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Organization: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*From: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Subject: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Nntp-Posting-Host: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Article-I\\.D\\.: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Keywords: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*In article <.*> you write:\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'wrote:$', '', text, flags=re.MULTILINE) # Common quote intro\n\n\n    # Truncate text to 5,000 characters to limit processing size\n    text = text.strip()[:5000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset, apply_clean):\n    \"\"\"\n    Converts the newsgroup dataset into a structured DataFrame:\n    - Stores text and labels.\n    - Cleans text using preprocess_newsgroup_row() if apply_clean is True.\n    - Maps numeric labels to category names.\n\n    Args:\n        newsgroup_dataset (sklearn.utils.Bunch): Newsgroup dataset object.\n        apply_clean (boolean) : Apply row cleaning\n\n    Returns:\n        pd.DataFrame: Preprocessed dataset with 'Text', 'Label', 'Class Name'.\n    \"\"\"\n    # Convert dataset into a pandas DataFrame\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    if apply_clean:\n        print(f\"Applying text cleaning...\")\n        # Apply text cleaning to each entry\n        df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n        # Remove rows where text became empty after cleaning\n        df = df[df[\"Text\"].str.strip().astype(bool)]\n        print(f\"DataFrame shape after cleaning: {df.shape}\")\n\n\n    # Convert numerical labels to category names\n    target_names = newsgroup_dataset.target_names\n    if target_names:\n         # Ensure label exists in target_names mapping\n         df[\"Class Name\"] = df[\"Label\"].apply(lambda l: target_names[l] if 0 <= l < len(target_names) else \"Unknown\")\n    else:\n         df[\"Class Name\"] = \"Unknown\"\n\n\n    return df.reset_index(drop=True) # Reset index after potential row removal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"14896fff","cell_type":"markdown","source":"#### create pandas  dataframe from training and test datasets ","metadata":{}},{"id":"42ea0147","cell_type":"code","source":"# Create pandas dataframes from training and test datasets *with* preprocessing\n# Check if raw data was loaded successfully\nif 'newsgroups_train_raw' in locals() and 'newsgroups_test_raw' in locals():\n    print(\"Preprocessing raw train data...\")\n    df_train_full = preprocess_newsgroup_data(newsgroups_train_raw, apply_clean=True)\n    print(\"\\nPreprocessing raw test data...\")\n    df_test_full = preprocess_newsgroup_data(newsgroups_test_raw, apply_clean=True)\n\n    print(\"\\nFull Train DataFrame head:\")\n    print(df_train_full.head())\n    print(f\"\\nFull Train DataFrame shape: {df_train_full.shape}\")\n    print(f\"Full Test DataFrame shape: {df_test_full.shape}\")\n\nelse:\n    print(\"ERROR: Raw data not loaded earlier: Cannot preprocess.\")\n    # Create empty dataframes to avoid subsequent errors, but notebook won't work\n    df_train_full = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])\n    df_test_full = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5a1f42a8","cell_type":"markdown","source":"#### Sampling Data to Define \"Normal\" Class ( to differentiate from Anamoly)\n\n*  We sample a subset of the data to represent our \"normal\" dataset for anomaly detection.\n*  Here, we choose the `sci` categories.\n*   We also sample the test set similarly to have a baseline of expected 'normal' test points.","metadata":{}},{"id":"cb8684a0","cell_type":"code","source":"# Function to sample data\ndef sample_data(df, num_samples_per_class, classes_to_keep_pattern):\n    \"\"\"\n    Samples rows from the dataset based on the specified number of samples per label\n    and filters the dataset to keep only specified categories matching a pattern. Handles cases where classes have fewer samples than requested.\n\n    Args:\n        df (pd.DataFrame): Input dataframe containing the data ('Text', 'Label', 'Class Name').\n        num_samples_per_class (int): Max number of samples to take per label.\n        classes_to_keep_pattern (str): Substring pattern to filter class names.\n\n    Returns:\n        pd.DataFrame: Filtered and sampled dataframe. Returns empty if no matching classes or input df is empty.\n    \"\"\"\n    if df.empty:\n         # print(\"Debug: Input DataFrame to sample_data is empty.\") # Removed Debug\n         return df\n\n    # Filter rows based on the pattern first\n    df['Class Name'] = df['Class Name'].astype(str)\n    try:\n        filter_mask = df[\"Class Name\"].str.contains(classes_to_keep_pattern, na=False, regex=False)\n        df_filtered = df[filter_mask].copy()\n    except Exception as filter_e:\n        print(f\"Error filtering DataFrame by pattern '{classes_to_keep_pattern}': {filter_e}\")\n        return pd.DataFrame(columns=df.columns) # Return empty on error\n\n\n    if df_filtered.empty:\n        print(f\"Warning: No classes found containing pattern '{classes_to_keep_pattern}' AFTER filtering.\")\n        return df_filtered\n\n    print(f\"Found classes containing '{classes_to_keep_pattern}': {df_filtered['Class Name'].unique().tolist()}\")\n    print(f\"Number of samples before sampling (after filtering): {len(df_filtered)}\")\n\n\n    # Sample rows, selecting num_samples_per_class of each remaining label\n    try:\n        # Ensure the sampling function handles empty groups if any arise\n        df_sampled = (\n            df_filtered.groupby(\"Class Name\", group_keys=False)\n            # Handle potential deprecation warning by explicitly selecting columns if needed,\n            # but standard apply should work. Add random_state for reproducibility.\n            .apply(lambda x: x.sample(min(num_samples_per_class, x.shape[0]), random_state=42) if not x.empty else None)\n        )\n        # Remove potential None results if a group was empty\n        if df_sampled is not None:\n             # Check if df_sampled is a DataFrame before calling dropna\n             if isinstance(df_sampled, pd.DataFrame):\n                  df_sampled.dropna(inplace=True)\n             else: # If apply returned something else (like Series if only one group)\n                  print(f\"Warning: Unexpected result type from groupby.apply: {type(df_sampled)}\")\n                  # Attempt to convert back to DataFrame or handle appropriately\n                  # For simplicity, return empty if structure is unexpected\n                  df_sampled = pd.DataFrame(columns=df_filtered.columns)\n\n        else: # Handle case where apply returns None (e.g., only one empty group)\n             df_sampled = pd.DataFrame(columns=df_filtered.columns)\n\n\n    except Exception as e:\n         print(f\"Error during sampling: {e}\")\n         return pd.DataFrame(columns=df.columns) # Return empty df on error\n\n    print(f\"Number of samples after sampling: {len(df_sampled)}\")\n    return df_sampled.reset_index(drop=True) # Reset index after sampling\n\n# --- Define constants (keep as before) ---\nTRAIN_NUM_SAMPLES_NORMAL = 100\nTEST_NUM_SAMPLES_NORMAL = 25\nNORMAL_CLASSES_PATTERN = \"sci\" # Keep classes containing 'sci' (sci.crypt, sci.electronics, sci.med, sci.space)\n\n# Initialize empty DataFrames for results\ndf_train = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])\ndf_test = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])\n\n# --- Sample 'normal' training data ---\nprint(\"\\n--- Preparing to sample TRAINING data ---\")\nif 'df_train_full' in locals() and not df_train_full.empty:\n    print(f\"\\nSampling 'normal' training data (pattern: {NORMAL_CLASSES_PATTERN})...\")\n    df_train = sample_data(df_train_full, TRAIN_NUM_SAMPLES_NORMAL, NORMAL_CLASSES_PATTERN)\n    if not df_train.empty:\n         print(f\"\\n'Normal' Training samples per class:\\n{df_train['Class Name'].value_counts()}\")\n    else:\n         print(\"Resulting 'normal' training DataFrame (df_train) is empty after sampling.\")\nelse:\n    print(\"Skipping training data sampling as df_train_full is empty or not defined.\")\n\n\n# --- Sample 'normal' test data ---\nprint(\"\\n--- Preparing to sample TEST data ---\")\nif 'df_test_full' in locals() and not df_test_full.empty:\n    print(f\"\\nSampling 'normal' test data (pattern: {NORMAL_CLASSES_PATTERN})...\")\n    df_test = sample_data(df_test_full, TEST_NUM_SAMPLES_NORMAL, NORMAL_CLASSES_PATTERN)\n    if not df_test.empty:\n         print(f\"\\n'Normal' Test samples per class:\\n{df_test['Class Name'].value_counts()}\")\n    else:\n         print(\"Resulting 'normal' test DataFrame (df_test) is empty after sampling.\")\n\nelse:\n     print(\"Skipping test data sampling as df_test_full is empty or not defined.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73090f29","cell_type":"markdown","source":"## Create the Embeddings\n\nGenerate embeddings for each piece of text using the Gemini API embeddings endpoint. ","metadata":{}},{"id":"1bc03877","cell_type":"markdown","source":"### Task types\n\nThe `text-embedding-004` model supports a `task_type` parameter that generates embeddings tailored for the specific task. For general similarity or anomaly detection based on topic, `RETRIEVAL_DOCUMENT` or `CLUSTERING` might also be suitable, but we'll stick with `CLASSIFICATION` as used previously, assuming it captures general semantic meaning well.","metadata":{}},{"id":"70467d3c","cell_type":"code","source":"from google.api_core import retry  # Import retry mechanism for handling API errors\nimport tqdm  # Import tqdm for progress bars\nfrom tqdm.rich import tqdm as tqdmr  # Rich progress bars for better visualization\nimport warnings  # Suppress warnings where necessary\n\n# Add tqdm to Pandas for progress tracking in DataFrame operations\ntqdmr.pandas()\n\n# Suppress experimental warnings from tqdm library\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Define a helper function to retry API calls when quota limits are reached\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable, timeout=300)  # Retry on specific API errors with a timeout of 300 seconds\ndef embed_fn(text: str) -> list[float]:\n    \"\"\"\n    Generates embeddings for a given text using an embedding model.\n\n    Args:\n        text (str): Input text to generate embeddings for.\n\n    Returns:\n        list[float]: Embedding vector as a list of floats.\n    \"\"\"\n    response = client.models.embed_content(\n        model=\"models/text-embedding-004\",  # Specify the embedding model to use\n        contents=text,  # Input text content from DF text column\n        config=types.EmbedContentConfig(\n            task_type=\"classification\",  # Specify task type (e.g., classification)\n        ),\n    )\n\n    return response.embeddings[0].values  # Return the embedding vector\n\ndef create_embeddings(df):\n    \"\"\"\n    Adds an 'Embeddings' column to the DataFrame by generating embeddings from text.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame with a 'Text' column.\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional 'Embeddings' column.\n    \"\"\"\n    df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_fn)  # Apply embedding generation with progress tracking\n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"476d3c01","cell_type":"code","source":"df_train = create_embeddings(df_train)\ndf_test = create_embeddings(df_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"16980137","cell_type":"markdown","source":"## Anomaly Detection Setup\n\nNow we set up the anomaly detection task:\n1.  **Create Synthetic Anomalies**: Define text examples representing categories completely different from the 'normal' (`sci.*`) newsgroup data, such as financial transactions and spam emails.\n2.  **Embed Anomalies**: Generate embeddings for these synthetic samples using the parallel embedding function.\n3.  **Combine**: Add the synthetic anomalous samples to the 'normal' test set (`df_test`).\n4.  **Calculate Distances**: Compute the distance (e.g., cosine distance) of each point in the combined test set from the centroid (average embedding) of the 'normal' training set (`df_train`).\n5.  **Identify Outliers**: Flag points with distances exceeding a threshold as anomalies. These should ideally be our synthetic examples.","metadata":{}},{"id":"535c9ce9","cell_type":"code","source":"# Ensure df_train and df_test are not empty before proceeding\nif df_train.empty or df_test.empty:\n     print(\"ERROR: 'Normal' train or test DataFrame is empty. Cannot proceed with anomaly detection.\")\nelse:\n     # --- 1. Create Synthetic Anomalous Samples ---\n     print(\"Creating synthetic anomaly samples...\")\n\n     # Define synthetic texts for different categories\n     # 5 Financial examples\n     financial_texts = [\n          \"URGENT: Your account statement for March is ready. Payment due April 15th. Click here to view details.\",\n          \"Stock Alert: ACME Corp (ACME) up 5% pre-market trading following positive earnings report.\",\n          \"Transaction Confirmation: $150.75 paid to 'OnlineRetailer'. Your new balance is $1,234.56.\",\n          \"Loan Application Update: Your mortgage pre-approval has been processed. A loan officer will contact you shortly.\",\n          \"Investment Opportunity: Learn about our new high-yield savings account with competitive APY rates.\"\n     ]\n     # 5 Spam examples\n     spam_texts = [\n          \"Congratulations! You've won a FREE iPhone 15! Click HERE to claim your prize NOW!\",\n          \"VIAGRA special offer!! Cheap pills online, discrete shipping guaranteed. Limited time only!\",\n          \"Urgent account security warning! Verify your login details immediately by clicking this link: http://totally-not-a-scam-site.com\",\n          \"Meet hot singles in your area tonight! Join free now, easy registration, instant matches!\",\n          \"Make $1000s working from home! Easy online job opportunity, no experience needed. Apply today!\"\n     ]\n\n     synthetic_texts = financial_texts + spam_texts\n     synthetic_labels = (['Financial'] * 5) + (['Spam'] * 5)\n     num_anomalies_to_sample = len(synthetic_texts) # Should be 10\n\n     # Create DataFrame for anomalies\n     df_anomalies = pd.DataFrame({\n          'Text': synthetic_texts,\n          'Class Name': synthetic_labels,\n          # Add dummy Label if needed by downstream code, though not strictly necessary for detection\n          'Label': [-1] * num_anomalies_to_sample # Assign a dummy label like -1\n     })\n\n     print(f\"Created {num_anomalies_to_sample} synthetic anomaly samples.\")\n     print(f\"Synthetic classes: {df_anomalies['Class Name'].unique().tolist()}\")\n\n\n     # --- 2. Generate Embeddings for Anomalies ---\n     print(\"\\nGenerating embeddings for synthetic anomalies...\")\n     df_anomalies = create_embeddings(df_anomalies) # Uses 'Text' column by default\n\n     # Check if embeddings were generated successfully for anomalies\n     if 'Embeddings' not in df_anomalies.columns or df_anomalies.empty:\n          print(\"Error: Failed to generate embeddings for synthetic anomaly samples. Cannot proceed.\")\n          # Handle error\n     else:\n          print(f\"Successfully generated embeddings for {len(df_anomalies)} synthetic anomaly samples.\")\n\n          # --- 3. Combine Anomalies with Test Set ---\n          # Add a flag to distinguish anomalies\n          df_anomalies['Is_Anomaly'] = True\n          # Add flag to original test set (ensure it hasn't been added before)\n          if 'Is_Anomaly' not in df_test.columns:\n               df_test['Is_Anomaly'] = False\n          else: # Ensure existing flags are False for the normal test set\n              df_test['Is_Anomaly'] = False\n\n\n          # Combine the original test set with the new anomalies\n          df_test_combined = pd.concat([df_test, df_anomalies], ignore_index=True)\n          print(f\"\\nCombined test set size: {len(df_test_combined)} rows\")\n\n          # --- 4. Detect Anomalies using Distance from Training Centroid ---\n          print(\"Calculating training centroid and distances...\")\n\n          # Ensure training embeddings are ready\n          if 'Embeddings' not in df_train.columns or df_train.empty:\n               print(\"Error: Training embeddings not available. Cannot calculate centroid.\")\n          else:\n                x_train_embeddings = np.stack(df_train['Embeddings'].values)\n                # Ensure combined test embeddings are ready (check after concat might be needed if anomalies failed)\n                if 'Embeddings' not in df_test_combined.columns or df_test_combined['Embeddings'].isnull().any():\n                     print(\"Warning: Missing embeddings in combined test set after concat. Dropping rows with missing embeddings.\")\n                     df_test_combined.dropna(subset=['Embeddings'], inplace=True)\n                     print(f\"Proceeding with {len(df_test_combined)} rows in combined test set.\")\n\n\n                if not df_test_combined.empty: # Check if still have data after potential drop\n                     x_test_combined_embeddings = np.stack(df_test_combined['Embeddings'].values)\n\n                     # Calculate the centroid (mean embedding) of the 'normal' training data\n                     train_centroid = np.mean(x_train_embeddings, axis=0)\n\n                     # Calculate cosine distance from each point in the combined test set to the training centroid\n                     distances = cosine_distances(x_test_combined_embeddings, train_centroid.reshape(1, -1))\n                     df_test_combined['Distance_to_Centroid'] = distances.flatten()\n\n                     # --- 5. Identify & Save Anomalies ---\n                     # Determine a threshold (e.g., 90th percentile of distances)\n                     valid_distances = df_test_combined['Distance_to_Centroid'].dropna()\n                     if not valid_distances.empty:\n                          # Calculate percentile based on expected number of anomalies\n                          # num_total = len(df_test_combined)\n                          # anomaly_percentile = (1 - (num_anomalies_to_sample / num_total)) * 100 if num_total > 0 else 90\n                          # Using fixed 90th percentile for simplicity\n                          anomaly_percentile = 90\n                          distance_threshold = np.percentile(valid_distances, anomaly_percentile)\n                          print(f\"Using distance threshold ({anomaly_percentile:.0f}th percentile): {distance_threshold:.4f}\")\n\n                          # Flag potential anomalies based on the threshold\n                          df_test_combined['Detected_Anomaly'] = df_test_combined['Distance_to_Centroid'] > distance_threshold\n\n                          # Separate the detected anomalies\n                          detected_anomalies_df = df_test_combined[df_test_combined['Detected_Anomaly'] == True].copy() # Explicit check for True\n\n                          print(\"\\n--- Detected Anomalies ---\")\n                          if not detected_anomalies_df.empty:\n                               # Display info about detected anomalies\n                               print(detected_anomalies_df[['Text', 'Class Name', 'Is_Anomaly', 'Distance_to_Centroid']].head())\n\n                               # Compare actual injected vs detected\n                               # Recalculate num_anomalies_present in the potentially filtered df_test_combined\n                               num_injected_present = df_test_combined['Is_Anomaly'].sum()\n                               correctly_detected = detected_anomalies_df['Is_Anomaly'].sum()\n                               false_positives = len(detected_anomalies_df) - correctly_detected\n                               print(f\"\\nTotal detected: {len(detected_anomalies_df)}\")\n                               print(f\"Correctly identified injected anomalies: {correctly_detected}/{num_injected_present} (Expected {num_anomalies_to_sample} injected initially)\")\n                               print(f\"Incorrectly identified (false positives): {false_positives}\")\n\n                               # Save the detected anomalies to a CSV\n                               try:\n                                    anomaly_filename = 'detected_synthetic_anomalies.csv'\n                                    # Select columns to save\n                                    columns_to_save = ['Text', 'Class Name', 'Is_Anomaly', 'Distance_to_Centroid', 'Detected_Anomaly', 'Embeddings']\n                                    detected_anomalies_df[columns_to_save].to_csv(anomaly_filename, index=False)\n                                    print(f\"\\nDetected anomalies saved to {anomaly_filename}\")\n                               except Exception as e:\n                                    print(f\"\\nError saving anomalies to CSV: {e}\")\n                          else:\n                               print(\"No anomalies detected above the threshold.\")\n                     else:\n                          print(\"Error: No valid distances found to calculate threshold.\")\n\n                     # Display the head of the combined test df showing new columns\n                     print(\"\\n--- Head of Combined Test Set with Anomaly Info ---\")\n                     print(df_test_combined[['Text', 'Class Name', 'Is_Anomaly', 'Distance_to_Centroid', 'Detected_Anomaly']].head())\n                else:\n                    print(\"Combined test set is empty after handling missing embeddings.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2261ccfd-be19-4841-aaec-a8e2c022468f","cell_type":"markdown","source":"---\n---","metadata":{}},{"id":"ef47f4b4","cell_type":"markdown","source":"## Jupyter Notebook to PDF/HTML Converter with Gradio\n\nThis project demonstrates a practical application combining:\n* Gradio for creating a user-friendly UI.\n* Python's `nbconvert` library via subprocess handling for converting notebooks to PDF or HTML format.\n\n#### Key Features:\n* Converts single notebooks or entire directories.\n* Supports **PDF** (local default) and **HTML** (Kaggle default/local option) output formats.\n* Intuitive **form-based interface**.\n* **Environment-aware processing** (adapts output format and path for Kaggle vs local environments).\n* **Safe file handling** using temporary copies to prevent conversion errors.\n* Automatic dependency checking and installation attempt for `nbconvert`.\n* Configurable output directory and overwrite options.\n\n#### Dependencies:\n* `gradio`\n* `google-genai` (Required for API key check, though Gemini chat is removed)\n* `jupyter` and `nbconvert[webpdf]` (Installs necessary components for PDF/HTML)\n* `python-dotenv` (for local API key loading)","metadata":{"id":"bhT1u-Pof10V"}},{"id":"7751a640","cell_type":"markdown","source":"#### 1.Install necessary software\n","metadata":{}},{"id":"b4bb6a41","cell_type":"code","source":"!pip install -U -q \"google-genai==1.7.0\"\n!pip install python-dotenv gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4a364fb8","cell_type":"code","source":"import os # Ensure os is imported\ndef is_running_on_kaggle():\n    \"\"\"Check if the code is running on Kaggle by looking for a specific environment variable.\"\"\"\n    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7f792795","cell_type":"code","source":"if is_running_on_kaggle():\n    base_op_folder_path = os.path.join(os.getcwd(), \"GooglCapstone\")\n     # Output format for Kaggle\n    output_format = \"html\"\n    # Define the specific output path for HTML files on Kaggle\n    html_output_path = os.path.join(base_op_folder_path, f\"converted_{output_format}\")\n    # Create the specific output directory\n    os.makedirs(html_output_path, exist_ok=True)\n    # Confirm the specific HTML folder exists\n    if os.path.exists(html_output_path):\n       print(f'Kaggle HTML output folder ready at: {html_output_path}')\n    else:\n       print(f'Error: Failed to create Kaggle HTML output folder at {html_output_path}')\n    # check and display input path\n    all_files_recursive = []\n    try:\n        for dirpath, dirnames, filenames in os.walk(\"/kaggle/input\"):\n            # To get full paths, join the current directory path with the filename\n            for filename in filenames:\n                full_path = os.path.join(dirpath, filename)\n                all_files_recursive.append(full_path)\n    \n        print(\"All files found recursively under /kaggle/input:\")\n        # Print each file path on a new line for readability\n        for f in all_files_recursive:\n            print(f)\n    \n    except Exception as e:\n            # os.walk itself doesn't raise FileNotFoundError if the top dir is missing,\n            # it just yields nothing. Check access or other errors.\n            print(f\"An error occurred during os.walk: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"544c5613","cell_type":"code","source":"import gradio as gr\nimport os\nimport subprocess\nimport sys\nimport pkg_resources\nimport re\nimport shutil\nimport re\nimport tempfile # Used for safe copies\nfrom google import genai\n# # Used for modern Gradio chat format\nfrom gradio import ChatMessage ","metadata":{"id":"FXq0ygI3BCdQ","trusted":true},"outputs":[],"execution_count":null},{"id":"3674354d","cell_type":"markdown","source":"#### 2. Set up a retry helper.\n*  This allows you to \"Run all\" without worrying about per-minute quota.\n*  Include  common HTTP status codes associated with temporary server issues, the `is_retriable` lambda function will identify a broader range of errors that are likely to be resolved by a retry attempt.","metadata":{}},{"id":"1296f11c","cell_type":"code","source":"from google.api_core import retry\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 500, 502, 503}) \ngenai.models.Models.generate_content = retry.Retry( predicate=is_retriable)(genai.models.Models.generate_content)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"45876d3a","cell_type":"markdown","source":"3. #### API Key Management and Google Client Initialization\nThis code handles the secure retrieval of the Google API key needed for Gemini API access. It works in both Kaggle and local environments:\n\n* **Purpose**: Securely load the Google API key without hardcoding it\n* **Environment Detection**: Automatically detects whether running on Kaggle or locally\n* **Key Features**:\n  * Uses Kaggle Secrets when running in Kaggle notebooks\n  * Falls back to environment variables when running locally\n  * Provides clear error messages if the key is missing\n  \n##### How It Works:\n\n* The `get_api_key()` function:\n  * Checks whether code is running on Kaggle by looking for 'KAGGLE_KERNEL_RUN_TYPE' in environment variables\n  * If on Kaggle: Uses Kaggle's UserSecretsClient to securely access the stored API key\n  * If running locally: \n    * Loads variables from .env file using dotenv\n    * Executes a batch script (`set_api_key.bat`)for Windows environments. This will Set environment variable to the GOOGLE API key.Alternatw way is to set the \n      system variable `GOOGLE_API_KEY` to the API key in Enviorment variable.\n    * Retrieves key from environment variables\n    * Raises helpful error messages if key isn't found\n\n##### Security Best Practices:\n\n* Never hardcodes API keys in the notebook\n* Uses platform-specific secure storage methods\n* Allows for flexible deployment across environments\n\nThis pattern is essential for any project using external APIs that require authentication while maintaining security.","metadata":{"id":"_mwJYXpElYJc"}},{"id":"d2e1dbca","cell_type":"code","source":"import os ,sys, subprocess,pkg_resources\ndef get_api_key():\n    \"\"\"\n    Retrieves the Google API key, attempting to get it from Kaggle Secrets\n    if running on Kaggle, otherwise from an environment variable.\n    \"\"\"\n    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n        # Running on Kaggle\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n        print(\"Running on Kaggle, API key loaded from Kaggle Secrets.\")\n        return api_key\n    else:\n\n        # Running locally\n        from dotenv import load_dotenv\n        load_dotenv()  # Add this line after the import\n        # Execute the batch script\n        subprocess.call(['set_api_key.bat'])\n        # Now load the environment variable\n        api_key = os.environ.get('GOOGLE_API_KEY')\n        # os.environ['GOOGLE_API_KEY'] = 'AIzaSyCNtEfGLS4qFdIbvoOxjSrOCzkGyUHk_kY'\n        # api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        print(api_key)\n        if api_key:\n            print(\"Running locally, API key loaded from environment variable.\")\n            return api_key\n        else:\n            raise EnvironmentError(\n                \"GOOGLE_API_KEY environment variable not set. \"\n                \"Please set it before running locally.\"\n            )\n# Get the API key\nGOOGLE_API_KEY = get_api_key()\n# Initialize the generative AI client\n","metadata":{"id":"tayrk_A2lZ7A","trusted":true},"outputs":[],"execution_count":null},{"id":"137f9fc4","cell_type":"code","source":"# setup the Client with the API key\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"94c082dd","cell_type":"markdown","source":"#### 4. Automatic nbConvert Installation\n\n*  A key challenge for notebook converters is ensuring all dependencies are properly installed.\n*   The function below (`check_and_install_dependencies`) checks if `nbconvert` is installed and attempts to install it with the `webpdf` extras if it's missing.\n*   The `webpdf` extras include dependencies required for PDF conversion (like Playwright/Pyppeteer), although this app primarily uses HTML conversion on Kaggle.\n\nThis function demonstrates **automation and error handling** for better user experience.","metadata":{}},{"id":"1be31845","cell_type":"code","source":"def check_and_install_dependencies():\n    \"\"\"\n    Checks if nbconvert[webpdf] is installed. If not, attempts to install it.\n    \n    Returns:\n        bool: True if dependencies are met or installed successfully, False otherwise.\n    \n    This function performs several steps:\n    1. Checks if nbconvert is already installed\n    2. If not, attempts to install it using pip\n    3. Verifies successful installation\n    4. Handles potential errors during installation\n    \"\"\"\n    try:\n        # Check if nbconvert is already installed\n        pkg_resources.get_distribution('nbconvert')\n        print(\"âœ“ nbconvert is already installed.\")\n        return True\n    except pkg_resources.DistributionNotFound as e:\n        # nbconvert not found, attempt installation\n        print(f\"Dependency missing: {e}. Attempting installation...\")\n        try:\n            # Use sys.executable to ensure pip runs in the correct Python environment\n            install_command = [sys.executable, \"-m\", \"pip\", \"install\", \"nbconvert[webpdf]\"]\n            print(f\"Running: {' '.join(install_command)}\")\n            \n            # Execute the installation command\n            result = subprocess.run(install_command, check=True, capture_output=True, text=True)\n            print(\"âœ“ Installation successful!\")\n            print(result.stdout)\n            \n            # Verify installation was successful\n            pkg_resources.get_distribution('nbconvert') \n            return True\n        except subprocess.CalledProcessError as install_error:\n            # Installation failed - provide detailed error info\n            print(\"-------------------- Installation Failed --------------------\")\n            print(f\"Error installing nbconvert[webpdf]: {install_error}\")\n            print(\"STDERR:\")\n            print(install_error.stderr)\n            print(\"STDOUT:\")\n            print(install_error.stdout)\n            print(\"-------------------------------------------------------------\")\n            print(\"Please try installing manually: pip install nbconvert[webpdf]\")\n            return False\n        except pkg_resources.DistributionNotFound:\n            # Installation seemed to succeed but verification failed\n            print(\"Verification failed after installation attempt.\")\n            return False\n        except Exception as general_error:\n            # Catch any other unexpected errors\n            print(f\"An unexpected error occurred during installation: {general_error}\")\n            return False\n    except Exception as check_error:\n        # Catch any other errors during the initial check\n        print(f\"Error checking dependencies: {check_error}\")\n        return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a9cd850","cell_type":"markdown","source":"#### 5. Safe Copy Handling for Notebooks\n\nSometimes, if the script converting a notebook is running from the same notebook file, it can lead to errors during conversion (`nbconvert` reading/writing the file simultaneously).\n\nTo mitigate this, these functions create safe copies of the target notebook(s) in a **temporary system directory** before conversion. This ensures the conversion process operates on a separate, static copy, which is automatically cleaned up afterwards.\n\n* `create_safe_copy`: Handles single files.\n* `create_safe_copies_from_dir`: Handles all `.ipynb` files in a directory.","metadata":{}},{"id":"c3363996","cell_type":"code","source":"def create_safe_copy(file_path, use_temp_dir=False):\n    \"\"\"\n    Creates a copy of a notebook file in a safe location to prevent pipe errors.\n    \n    Args:\n        file_path (str): Path to the original notebook file\n        use_temp_dir (bool): If True, use system temp directory, otherwise use 'ip_folder'\n        \n    Returns:\n        str: Path to the copied file\n    \"\"\"\n    if not os.path.exists(file_path) or not file_path.endswith('.ipynb'):\n        return file_path  # Return original if not a valid notebook\n        \n    # Get directory and filename\n    original_dir = os.path.dirname(file_path)\n    filename = os.path.basename(file_path)\n    \n    if use_temp_dir:\n        # Use system temp directory\n        import tempfile\n        temp_dir = tempfile.mkdtemp()\n        copy_path = os.path.join(temp_dir, filename)\n    else:\n        # Create ip_folder in the same directory as the original file\n        copy_dir = os.path.join(original_dir, \"ip_folder\")\n        os.makedirs(copy_dir, exist_ok=True)\n        copy_path = os.path.join(copy_dir, filename)\n    \n    # Copy the file\n    import shutil\n    shutil.copy2(file_path, copy_path)\n    print(f\"Created copy of {file_path} at {copy_path}\")\n    \n    return copy_path\n\ndef create_safe_copies_from_dir(dir_path, use_temp_dir=False):\n    \"\"\"\n    Creates copies of all notebook files in a directory in a safe location.\n    \n    Args:\n        dir_path (str): Path to the directory containing notebooks\n        use_temp_dir (bool): If True, use system temp directory, otherwise use 'ip_folder'\n        \n    Returns:\n        tuple: (directory path of copies, list of copied file paths)\n    \"\"\"\n    if not os.path.isdir(dir_path):\n        return dir_path, []  # Return original if not a valid directory\n    \n    if use_temp_dir:\n        # Use system temp directory\n        import tempfile\n        copy_dir = tempfile.mkdtemp()\n    else:\n        # Create ip_folder in the same directory\n        copy_dir = os.path.join(dir_path, \"ip_folder\")\n        os.makedirs(copy_dir, exist_ok=True)\n    \n    # Copy all notebook files\n    import shutil\n    copied_files = []\n    \n    for filename in os.listdir(dir_path):\n        if filename.endswith('.ipynb'):\n            original_path = os.path.join(dir_path, filename)\n            copy_path = os.path.join(copy_dir, filename)\n            shutil.copy2(original_path, copy_path)\n            copied_files.append(copy_path)\n    \n    print(f\"Copied {len(copied_files)} notebook files to {copy_dir}\")\n    \n    return copy_dir, copied_files\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e62ac3f0","cell_type":"markdown","source":"#### 6. Core Notebook Conversion Functions\n\nThese functions handle the actual conversion from Jupyter notebooks to **PDF or HTML** format.\n\nThey are designed to:\n* Convert single notebooks (`convert_notebook_to_pdf`, `convert_notebook_to_html`) or all notebooks within a directory (`convert_all_notebooks_in_dir`).\n* Be called by the UI handler (`handle_conversion_request`), which automatically determines the correct format: **HTML on Kaggle**, or **PDF/HTML locally** based on user selection.\n* Utilize the **safe copy** functions to prevent errors when converting.\n* Manage output directories, placing converted files in a structured way (e.g., in `/kaggle/working/` on Kaggle or relative to input/specified path locally).\n* Provide clear status messages for success, failure, or skipped files (attempting to avoid self-conversion using `get_current_notebook_name`).\n* Support overwriting existing output files.\n* For HTML, support excluding code input cells via the `include_input` parameter.","metadata":{}},{"id":"530ee18d","cell_type":"code","source":"def get_current_notebook_name():\n    \"\"\"\n    Attempts to determine the filename of the currently running notebook.\n    Returns None if it cannot be determined.\n    (Based on the function in project1-goog-capstone.ipynb)\n    \"\"\"\n    try:\n        # Method 1: Try to get from IPython kernel information\n        from IPython import get_ipython\n        ipython = get_ipython()\n        if ipython and hasattr(ipython, 'kernel') and hasattr(ipython.kernel, 'session') and hasattr(ipython.kernel.session, 'notebook_path'):\n             # Path might be relative or absolute depending on environment\n             notebook_path = ipython.kernel.session.notebook_path\n             # We just need the filename part\n             current_file = os.path.basename(notebook_path)\n             # Basic check if it looks like a notebook file\n             if current_file.endswith('.ipynb'):\n                  print(f\"DEBUG: Found current notebook name (method 1 - IPython kernel): {current_file}\")\n                  return current_file\n\n        # Method 2: Inspect stack frames (less reliable in notebooks, but worth trying)\n        # Look for a frame filename ending in .ipynb\n        for frame_info in inspect.stack():\n            # frame_info structure can vary, access filename carefully\n            filename = frame_info.filename if hasattr(frame_info, 'filename') else getattr(frame_info, 'filename', None) # Adapt based on Python version/env\n            if filename and filename.lower().endswith('.ipynb'):\n                current_file = os.path.basename(filename)\n                print(f\"DEBUG: Found potential notebook name (method 2 - inspect stack): {current_file}\")\n                # This might pick up internal library files sometimes, add checks if needed\n                # For now, return the first one found\n                return current_file\n\n        # Method 3: Check sys.argv[0] (often works when running as script, less so in kernels)\n        if sys.argv[0].lower().endswith('.ipynb'):\n            current_file = os.path.basename(sys.argv[0])\n            print(f\"DEBUG: Found current notebook name (method 3 - sys.argv): {current_file}\")\n            return current_file\n\n        # Method 4: Check environment variables (might be set in some platforms)\n        # Example: Check for VS Code interactive window variable\n        vscode_nb_file = os.environ.get('VSCODE_NOTEBOOK_FILE')\n        if vscode_nb_file and vscode_nb_file.lower().endswith('.ipynb'):\n             current_file = os.path.basename(vscode_nb_file)\n             print(f\"DEBUG: Found current notebook name (method 4 - VSCODE_NOTEBOOK_FILE): {current_file}\")\n             return current_file\n        # Add checks for other platform-specific variables if needed\n\n        print(\"DEBUG: Could not determine current notebook name using available methods.\")\n\n    except Exception as e:\n        # Catch potential errors during introspection (e.g., permissions, environment issues)\n        print(f\"Note: An error occurred while trying to determine the current notebook name: {e}\")\n\n    # If none of the methods worked, return None\n    return None\n\n\ndef convert_notebook_to_pdf(notebook_path, final_output_dir, overwrite=False, use_safe_copy=True):\n    \"\"\"\n    Converts a single notebook to PDF in the specified output directory.\n\n    Args:\n        notebook_path (str): Full path to the .ipynb file to convert\n        final_output_dir (str): Directory where the PDF will be saved\n        overwrite (bool, optional): Whether to overwrite existing PDFs. Defaults to False.\n        use_safe_copy (bool, optional): Whether to create a safe copy to prevent pipe errors. Defaults to True.\n\n    Returns:\n        str: Status message indicating success or failure\n    \"\"\"\n    # Get the current notebook name to avoid attempting self-conversion issues\n    try:\n        current_notebook = get_current_notebook_name() # Assumes this helper exists\n        notebook_basename = os.path.basename(notebook_path)\n        if current_notebook and notebook_basename == current_notebook:\n            return f\"Skipped PDF conversion (matches currently running notebook): {notebook_path}\"\n    except NameError:\n        print(\"Note: get_current_notebook_name() helper not found, skipping self-conversion check for PDF.\")\n\n    # Validate notebook path\n    if not notebook_path or not os.path.exists(notebook_path) or not notebook_path.lower().endswith('.ipynb'):\n        return f\"Error: Invalid or non-existent notebook file path for PDF conversion: {notebook_path}\"\n\n    # Track original path for result message\n    original_path = notebook_path\n\n    # Create a safe copy if requested (helps prevent pipe errors)\n    temp_dir = None # Initialize temp_dir\n    if use_safe_copy:\n        try:\n            # Assumes create_safe_copy helper exists\n            notebook_path = create_safe_copy(notebook_path, use_temp_dir=True)\n            temp_dir = os.path.dirname(notebook_path) # Store the temp dir path\n            print(f\"Using safe copy for PDF conversion at: {notebook_path}\")\n        except NameError:\n             print(\"Warning: create_safe_copy() helper not found. Using original file for PDF conversion.\")\n             use_safe_copy = False # Disable cleanup logic if copy wasn't made\n        except Exception as e:\n            print(f\"Warning: Could not create safe copy for PDF conversion: {e}\")\n            use_safe_copy = False # Continue with original file, disable cleanup\n\n    # Ensure the output directory exists\n    try:\n        os.makedirs(final_output_dir, exist_ok=True)\n    except OSError as e:\n         # Clean up temp dir if creation failed early\n        if temp_dir and use_safe_copy and ('temp' in temp_dir.lower() or 'tmp' in temp_dir.lower()):\n            try: shutil.rmtree(temp_dir)\n            except Exception as clean_e: print(f\"Warning: Error cleaning up temp dir {temp_dir} after failed output dir creation: {clean_e}\")\n        return f\"Error: Could not create output directory {final_output_dir}: {e}\"\n\n    # Determine output PDF path and check for existing files\n    output_pdf_path = os.path.join(final_output_dir, os.path.basename(original_path).replace('.ipynb', '.pdf'))\n\n    if not overwrite and os.path.exists(output_pdf_path):\n        # Clean up temp dir if skipping\n        if temp_dir and use_safe_copy and ('temp' in temp_dir.lower() or 'tmp' in temp_dir.lower()):\n            try: shutil.rmtree(temp_dir)\n            except Exception as clean_e: print(f\"Warning: Error cleaning up temp dir {temp_dir} when skipping existing file: {clean_e}\")\n        return f\"Skipped (already exists): {output_pdf_path}\"\n\n    # Prepare the nbconvert command\n    command = [\n        sys.executable, \"-m\", \"jupyter\", \"nbconvert\",\n        \"--to\", \"webpdf\", \"--allow-chromium-download\", # webpdf format with Chromium download permission\n        \"--output-dir\", final_output_dir,\n        notebook_path # Use the (potentially copied) path\n    ]\n\n    try:\n        # Execute the conversion command\n        print(f\"Running command: {' '.join(command)}\")\n        # Increased timeout for potentially long PDF rendering\n        result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=600) # e.g., 10 minutes\n\n        # Log outputs for debugging\n        print(f\"nbconvert STDOUT:\\n{result.stdout}\")\n        if result.stderr:\n            print(f\"nbconvert STDERR:\\n{result.stderr}\")\n\n        # Verify successful conversion\n        if os.path.exists(output_pdf_path):\n            return f\"Success: Converted {original_path} to {output_pdf_path}\"\n        else:\n            # Check specific errors like Chromium issues\n            stderr_lower = result.stderr.lower() if result.stderr else \"\"\n            if \"chromium\" in stderr_lower and (\"download\" in stderr_lower or \"executable\" in stderr_lower or \"timeout\" in stderr_lower):\n                return f\"Warning: Command ran but output PDF not found. Chromium setup/download/timeout might have failed for {original_path}. Check logs.\"\n            elif \"timeout\" in stderr_lower:\n                 return f\"Error: PDF conversion process timed out internally for {original_path}. Check logs.\"\n            return f\"Warning: Command ran but output PDF not found for {original_path}. Check logs.\"\n    except subprocess.CalledProcessError as e:\n        # Command execution failed\n        print(f\"nbconvert Error Output: {e.stderr}\")\n        # Check for common errors in stderr\n        stderr_lower = e.stderr.lower() if e.stderr else \"\"\n        if \"pyppeteer\" in stderr_lower or \"playwright\" in stderr_lower:\n             return f\"Error converting {original_path}: Missing PDF dependency (Pyppeteer/Playwright). Ensure `nbconvert[webpdf]` is installed. Details: {e.stderr[:300]}...\"\n        elif \"timeout\" in stderr_lower:\n             return f\"Error converting {original_path}: Process timed out. Details: {e.stderr[:300]}...\"\n        return f\"Error converting {original_path}: nbconvert failed. Details: {e.stderr[:500]}...\"\n    except subprocess.TimeoutExpired:\n        # Outer command timeout\n        return f\"Error: nbconvert command timed out (over 600s) for {original_path}.\"\n    except FileNotFoundError:\n        # Jupyter command not found\n        return \"Error: 'jupyter' command not found. Is Jupyter installed and in PATH?\"\n    except Exception as e:\n        # Catch any other exceptions\n        return f\"An unexpected error occurred during PDF conversion: {e}\"\n    finally:\n        # Clean up temporary directory if one was created\n        if temp_dir and use_safe_copy and ('temp' in temp_dir.lower() or 'tmp' in temp_dir.lower()):\n            try:\n                shutil.rmtree(temp_dir)\n                print(f\"Cleaned up temporary directory: {temp_dir}\")\n            except Exception as e:\n                print(f\"Warning: Could not clean up temporary directory {temp_dir}: {e}\")\n\n\ndef convert_notebook_to_html(notebook_path, final_output_dir, overwrite=False, use_safe_copy=True, include_input=True):\n    \"\"\"\n    Converts a single notebook to HTML in the specified output directory.\n\n    Args:\n        notebook_path (str): Full path to the .ipynb file to convert\n        final_output_dir (str): Directory where the HTML will be saved\n        overwrite (bool, optional): Whether to overwrite existing HTML files. Defaults to False.\n        use_safe_copy (bool, optional): Whether to create a safe copy to prevent pipe errors. Defaults to True.\n        include_input (bool, optional): Whether to include code cells in the output. Defaults to True.\n\n    Returns:\n        str: Status message indicating success or failure\n    \"\"\"\n    # Get the current notebook name to avoid attempting self-conversion issues\n    try:\n        current_notebook = get_current_notebook_name() # Assumes this helper exists\n        notebook_basename = os.path.basename(notebook_path)\n        if current_notebook and notebook_basename == current_notebook:\n            return f\"Skipped HTML conversion (matches currently running notebook): {notebook_path}\"\n    except NameError:\n        print(\"Note: get_current_notebook_name() helper not found, skipping self-conversion check for HTML.\")\n\n    # Validate notebook path\n    if not notebook_path or not os.path.exists(notebook_path) or not notebook_path.lower().endswith('.ipynb'):\n        return f\"Error: Invalid or non-existent notebook file path for HTML conversion: {notebook_path}\"\n\n    # Track original path for result message\n    original_path = notebook_path\n\n    # Create a safe copy if requested\n    temp_dir = None # Initialize temp_dir\n    if use_safe_copy:\n        try:\n            # Assumes create_safe_copy helper exists\n            notebook_path = create_safe_copy(notebook_path, use_temp_dir=True)\n            temp_dir = os.path.dirname(notebook_path) # Store the temp dir path\n            print(f\"Using safe copy for HTML conversion at: {notebook_path}\")\n        except NameError:\n             print(\"Warning: create_safe_copy() helper not found. Using original file for HTML conversion.\")\n             use_safe_copy = False # Disable cleanup logic\n        except Exception as e:\n            print(f\"Warning: Could not create safe copy for HTML conversion: {e}\")\n            use_safe_copy = False # Continue with original file, disable cleanup\n\n    # Ensure the output directory exists\n    try:\n        os.makedirs(final_output_dir, exist_ok=True)\n    except OSError as e:\n        # Clean up temp dir if creation failed early\n        if temp_dir and use_safe_copy and ('temp' in temp_dir.lower() or 'tmp' in temp_dir.lower()):\n            try: shutil.rmtree(temp_dir)\n            except Exception as clean_e: print(f\"Warning: Error cleaning up temp dir {temp_dir} after failed output dir creation: {clean_e}\")\n        return f\"Error: Could not create output directory {final_output_dir}: {e}\"\n\n    # Determine output HTML path and check for existing files\n    output_html_path = os.path.join(final_output_dir, os.path.basename(original_path).replace('.ipynb', '.html'))\n\n    if not overwrite and os.path.exists(output_html_path):\n        # Clean up temp dir if skipping\n        if temp_dir and use_safe_copy and ('temp' in temp_dir.lower() or 'tmp' in temp_dir.lower()):\n            try: shutil.rmtree(temp_dir)\n            except Exception as clean_e: print(f\"Warning: Error cleaning up temp dir {temp_dir} when skipping existing file: {clean_e}\")\n        return f\"Skipped (already exists): {output_html_path}\"\n\n    # Prepare the nbconvert command for HTML\n    command = [\n        sys.executable, \"-m\", \"jupyter\", \"nbconvert\",\n        \"--to\", \"html\",\n        # Add option to exclude input cells if requested\n        *([\"--no-input\"] if not include_input else []), # Nicer way to add optional arg\n        \"--output-dir\", final_output_dir,\n        notebook_path # Use the (potentially copied) path\n    ]\n\n    try:\n        # Execute the conversion command\n        print(f\"Running command: {' '.join(command)}\")\n        # Shorter timeout usually sufficient for HTML\n        result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=180)\n\n        # Log outputs for debugging\n        print(f\"nbconvert STDOUT:\\n{result.stdout}\")\n        if result.stderr:\n            print(f\"nbconvert STDERR:\\n{result.stderr}\")\n\n        # Verify successful conversion by checking expected output path\n        if os.path.exists(output_html_path):\n            return f\"Success: Converted {original_path} to {output_html_path}\"\n        else:\n            # Check if nbconvert output indicates a different filename (less likely for HTML)\n            stdout_output_match = re.search(r\"Writing .+ bytes to (.*\\.html)\", result.stdout)\n            if stdout_output_match and os.path.exists(stdout_output_match.group(1)):\n                 return f\"Success: Converted {original_path} to {stdout_output_match.group(1)}\"\n            else:\n                 return f\"Warning: Command ran but expected output HTML not found for {original_path}. Check logs.\"\n\n    except subprocess.CalledProcessError as e:\n        # Command execution failed\n        print(f\"nbconvert Error Output: {e.stderr}\")\n        return f\"Error converting {original_path} to HTML: {e.stderr[:500]}...\"\n    except subprocess.TimeoutExpired:\n        # Command took too long\n        return f\"Error: HTML conversion command timed out for {original_path}.\"\n    except FileNotFoundError:\n        # Jupyter command not found\n        return \"Error: 'jupyter' command not found. Is Jupyter installed and in PATH?\"\n    except Exception as e:\n        # Catch any other exceptions\n        return f\"An unexpected error occurred during HTML conversion: {e}\"\n    finally:\n        # Clean up temporary directory if one was created via safe copy\n        if temp_dir and use_safe_copy and ('temp' in temp_dir.lower() or 'tmp' in temp_dir.lower()):\n            try:\n                shutil.rmtree(temp_dir)\n                print(f\"Cleaned up temporary directory: {temp_dir}\")\n            except Exception as e:\n                print(f\"Warning: Could not clean up temporary directory {temp_dir}: {e}\")\n\n\ndef convert_all_notebooks_in_dir(input_dir, final_output_dir, overwrite=False, use_safe_copy=True, format=\"pdf\", include_input=True):\n    \"\"\"\n    Converts all notebooks in a directory to the specified format (PDF or HTML).\n\n    Args:\n        input_dir (str): Directory containing notebooks to convert\n        final_output_dir (str): Directory where output files will be saved\n        overwrite (bool, optional): Whether to overwrite existing files. Defaults to False.\n        use_safe_copy (bool, optional): Whether to use safe copies for conversion. Defaults to True.\n        format (str, optional): Output format, either \"pdf\" or \"html\". Defaults to \"pdf\".\n        include_input (bool, optional): Whether to include code cells in HTML output. Defaults to True.\n\n    Returns:\n        list: List of status messages for each notebook conversion\n    \"\"\"\n    results = []\n    # Get the current notebook name to avoid attempting self-conversion issues\n    try:\n        current_notebook = get_current_notebook_name() # Assumes this helper exists\n    except NameError:\n        print(\"Note: get_current_notebook_name() helper not found, skipping self-conversion check.\")\n        current_notebook = None\n\n    # Validate input directory\n    if not input_dir or not os.path.isdir(input_dir):\n        return [f\"Error: Invalid input directory: {input_dir}\"]\n\n    # Ensure output directory exists (create if needed)\n    try:\n        os.makedirs(final_output_dir, exist_ok=True)\n    except OSError as e:\n        return [f\"Error: Could not create output directory {final_output_dir}: {e}\"]\n\n    # Option 1: Create copies of all notebooks first (might use more temp space)\n    temp_dir_for_all = None\n    copied_notebooks_map = {} # Map copied path -> original path\n    if use_safe_copy:\n        try:\n            # Assumes create_safe_copies_from_dir helper exists\n            temp_dir_for_all, copied_paths = create_safe_copies_from_dir(input_dir, use_temp_dir=True)\n            if copied_paths:\n                # Create mapping from temp path back to original path for messages\n                for cp_path in copied_paths:\n                    original_filename = os.path.basename(cp_path)\n                    copied_notebooks_map[cp_path] = os.path.join(input_dir, original_filename)\n\n                print(f\"Processing {len(copied_notebooks_map)} notebooks from temporary copies in {temp_dir_for_all}\")\n                items_to_process = list(copied_notebooks_map.keys()) # Process the copied paths\n                process_with_individual_copies = False # Don't make another copy\n            else:\n                print(\"No notebooks found to copy. Processing originals.\")\n                items_to_process = os.listdir(input_dir) # Process original filenames\n                process_with_individual_copies = True # Need individual copies if processing originals\n\n        except NameError:\n            print(\"Warning: create_safe_copies_from_dir() helper not found. Will process original files with individual copies.\")\n            items_to_process = os.listdir(input_dir)\n            use_safe_copy = True # Ensure individual copy logic runs\n            process_with_individual_copies = True\n        except Exception as e:\n            print(f\"Warning: Could not create safe copies for directory: {e}. Will try processing originals with individual copies.\")\n            items_to_process = os.listdir(input_dir)\n            use_safe_copy = True # Ensure individual copy logic runs\n            process_with_individual_copies = True\n    else:\n         # Not using safe copies at all\n         print(\"Processing original files without safe copies.\")\n         items_to_process = os.listdir(input_dir)\n         process_with_individual_copies = False # No copies needed\n\n\n    # Process items (either copied paths or original filenames)\n    notebook_found_count = 0\n    for item_name in items_to_process:\n        # If processing originals, construct full path; otherwise, item_name is already full copied path\n        if process_with_individual_copies:\n             notebook_path = os.path.join(input_dir, item_name)\n             original_path_for_msg = notebook_path # Original path is the one being processed\n        else:\n             # item_name is the copied path, get original path for messages\n             notebook_path = item_name\n             original_path_for_msg = copied_notebooks_map.get(notebook_path, notebook_path) # Fallback to copied path if map fails\n\n        # Check if it's a notebook file\n        if notebook_path.lower().endswith(\".ipynb\"):\n             notebook_found_count += 1\n\n             # Skip if this is the currently running notebook\n             if current_notebook and os.path.basename(original_path_for_msg) == current_notebook:\n                 results.append(f\"Skipped (matches currently running notebook): {original_path_for_msg}\")\n                 continue\n\n             try:\n                 # Process each notebook individually\n                 if format.lower() == \"html\":\n                     # Pass process_with_individual_copies to decide if inner copy needed\n                     result = convert_notebook_to_html(notebook_path, final_output_dir, overwrite,\n                                                      use_safe_copy=process_with_individual_copies,\n                                                      include_input=include_input)\n                 else: # Default to PDF\n                     result = convert_notebook_to_pdf(notebook_path, final_output_dir, overwrite,\n                                                     use_safe_copy=process_with_individual_copies)\n\n                 # If processing copies, replace temp path in result with original path for clarity\n                 if not process_with_individual_copies and notebook_path in result:\n                      result = result.replace(notebook_path, original_path_for_msg)\n\n                 results.append(result)\n\n             except Exception as e:\n                 # Log the error but continue with other notebooks\n                 results.append(f\"Error processing {os.path.basename(original_path_for_msg)}: {str(e)}\")\n\n\n    if notebook_found_count == 0:\n        results.append(f\"No .ipynb files found in directory: {input_dir}\")\n\n    # Clean up the single temporary directory if one was created for all copies\n    if temp_dir_for_all and use_safe_copy and ('temp' in temp_dir_for_all.lower() or 'tmp' in temp_dir_for_all.lower()):\n         try:\n             shutil.rmtree(temp_dir_for_all)\n             print(f\"Cleaned up temporary directory for all copies: {temp_dir_for_all}\")\n         except Exception as e:\n             print(f\"Warning: Could not clean up directory {temp_dir_for_all}: {e}\")\n\n    return results\n\n# --- End of Core Conversion Functions ---","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"03bb79a7","cell_type":"markdown","source":"#### 8. Form-Based UI Handler (`handle_conversion_request`)\n\nThis function processes requests from the Gradio form interface. It acts as the bridge between the UI inputs and the core conversion functions.\n\nIt handles:\n* Single file uploads or directory path inputs.\n* Determining the correct output format (HTML on Kaggle, PDF/HTML locally).\n* Constructing the appropriate output directory path (using `/kaggle/working/` on Kaggle).\n* Passing user selections (overwrite, include code) to the conversion functions.\n* Formatting the results for display in the UI log, including separators for readability.","metadata":{}},{"id":"66340090","cell_type":"markdown","source":"","metadata":{}},{"id":"eb32a62f","cell_type":"code","source":"\ndef handle_conversion_request(input_path_type, single_file, input_dir, output_dir_base, overwrite, format=\"pdf\", include_input=True):\n    \"\"\"\n    Handles conversion requests from the form-based UI, adapting format for Kaggle,\n    and adding log separators for directory processing.\n\n    Args:\n        input_path_type (str): \"Single File\" or \"Directory\"\n        single_file (file object/temp file path): Uploaded file object (for Single File mode)\n        input_dir (str): Directory path containing notebooks (for Directory mode)\n        output_dir_base (str): Optional custom output directory path (base path)\n        overwrite (bool): Whether to overwrite existing files\n        format (str, optional): Preferred output format (\"pdf\" or \"html\"). Defaults to \"pdf\".\n        include_input (bool, optional): Whether to include code cells in HTML output. Defaults to True.\n\n    Returns:\n        str: Conversion results as a formatted string, with newlines for readability.\n    \"\"\"\n    results = []\n    final_output_dir = None # Initialize\n    try: # Wrap core logic in try/except for unexpected errors\n        # 1. Check Core Dependencies (nbconvert)\n        # Assumes check_and_install_dependencies() is defined elsewhere\n        if not check_and_install_dependencies():\n            return \"Dependency check/installation failed (nbconvert). Cannot proceed.\"\n\n        # 2. Determine Final Format based on Environment\n        final_format = format.lower()\n        # Assumes is_running_on_kaggle() is defined elsewhere\n        if is_running_on_kaggle() and final_format == \"pdf\":\n            final_format = \"html\"\n            results.append(\"Running on Kaggle - defaulting to HTML format.\")\n\n        # 3. Determine Final Output Directory Path\n        # Default base path = /kaggle/working on Kaggle, current dir otherwise\n        base_path_for_output = os.getcwd()\n\n        # Determine base path more intelligently\n        if input_path_type == \"Single File\" and single_file:\n            if hasattr(single_file, 'name') and isinstance(single_file.name, str):\n                 try:\n                     temp_dir = os.path.dirname(single_file.name)\n                     if os.path.isdir(temp_dir):\n                          base_path_for_output = temp_dir\n                 except Exception as e:\n                     print(f\"Could not use temp file dir '{single_file.name}' as base: {e}. Defaulting to CWD.\")\n        elif input_path_type == \"Directory\" and input_dir and os.path.isdir(input_dir):\n             # Use input dir as base *only if not Kaggle input*\n            if not (is_running_on_kaggle() and input_dir.startswith(\"/kaggle/input\")):\n                 base_path_for_output = input_dir\n\n        # Construct final path\n        if output_dir_base: # User specified a base directory for the output folder\n            final_output_dir = os.path.join(output_dir_base, f\"converted_{final_format}\")\n        else: # Default: use determined base path\n            final_output_dir = os.path.join(base_path_for_output, f\"converted_{final_format}\")\n\n        results.append(f\"Output directory set to: {final_output_dir}\")\n        os.makedirs(final_output_dir, exist_ok=True) # Ensure it exists\n\n        # 4. Process Conversion based on Input Type and Format\n        if input_path_type == \"Single File\":\n            if single_file and hasattr(single_file, 'name') and isinstance(single_file.name, str):\n                filepath = single_file.name # Path to the temporary uploaded file\n                results.append(f\"Processing file: {filepath}\")\n                # Assumes conversion functions are defined elsewhere\n                if final_format == \"html\":\n                    result = convert_notebook_to_html(filepath, final_output_dir, overwrite, use_safe_copy=True, include_input=include_input)\n                else:\n                    result = convert_notebook_to_pdf(filepath, final_output_dir, overwrite, use_safe_copy=True)\n                results.append(result)\n            else:\n                results.append(\"Error: No file uploaded or file path unavailable.\")\n\n        elif input_path_type == \"Directory\":\n            if input_dir and os.path.isdir(input_dir):\n                results.append(f\"Processing directory: {input_dir}\")\n\n                # --- Add separator line for readability ---\n                results.append(\"\\n--- File Conversion Status ---\")\n                # ------------------------------------------\n\n                # Assumes conversion function is defined elsewhere\n                conversion_results = convert_all_notebooks_in_dir(\n                    input_dir, final_output_dir, overwrite, use_safe_copy=True,\n                    format=final_format, include_input=include_input\n                )\n                # conversion_results is expected to be a list of strings\n\n                results.extend(conversion_results) # Add the list of individual results\n\n            else:\n                results.append(f\"Error: Invalid input directory path: {input_dir}\")\n\n    # --- Error Handling ---\n    except OSError as e:\n         results.append(f\"Error creating output directory '{final_output_dir}': {e}\")\n    except NameError as e:\n         results.append(f\"Error: A required helper function might be missing: {e}\")\n         print(f\"NameError in handle_conversion_request: {e}\")\n         traceback.print_exc()\n    except Exception as e:\n         results.append(f\"An unexpected error occurred: {e}\")\n         print(f\"Unexpected Error in handle_conversion_request: {e}\")\n         traceback.print_exc() # Print full traceback to console\n\n    # --- Return ---\n    # Join all accumulated messages (including the separator) with newlines\n    return \"\\\\n\".join(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2f069116","cell_type":"markdown","source":"#### 9. Chat Interface Message Processing (`process_chat_message`)\n\n*(This function handles the logic for the chat interface. It is defined here but the Gradio UI below **does not currently include the chat tab**, so this function is inactive unless the UI code is modified to add the chat components back.)*\n\nThis function represents the core of the AI-powered chat interface:\n\n* Parses user messages to detect conversion intent and extract details (paths, options).\n* Determines the correct output format (HTML on Kaggle, PDF/HTML locally).\n* Constructs appropriate output paths (using `/kaggle/working/` on Kaggle).\n* Calls the relevant conversion functions (`convert_notebook_to_...`, `convert_all_notebooks_in_dir`).\n* Manages conversation state (`chat_state`) to handle multi-turn interactions (like asking for a path).\n* Interacts with the Gemini AI model (`client.start_chat`) for non-conversion-related messages or general assistance.","metadata":{}},{"id":"f51d9fde","cell_type":"code","source":"def process_chat_message(message, chat_history, chat_state):\n    \"\"\"\n    Handles messages from the chat interface, processes conversion requests with environment awareness.\n\n    Args:\n        message (str): The user's message text.\n        chat_history (list): Current chat history (list of ChatMessage objects).\n        chat_state (dict): State dictionary containing context and the Gemini chat object.\n\n    Returns:\n        tuple: (updated chat_history, updated chat_state)\n    \"\"\"\n    # 1. Initialize chat state if needed\n    if chat_state is None:\n        chat_state = {\"gemini_chat\": init_chat(), \"context\": {}}\n\n    # 2. Skip empty messages\n    if not message.strip():\n        return chat_history, chat_state\n\n    user_message = message.strip()\n\n    # 3. Add user message to Gradio chat history (modern format)\n    chat_history.append(ChatMessage(role=\"user\", content=user_message))\n\n    # 4. --- Determine Conversion Parameters ---\n    conversion_intent = (\n        \"convert\" in user_message.lower() and\n        (\"notebook\" in user_message.lower() or \".ipynb\" in user_message.lower() or \"file\" in user_message.lower())\n    )\n    overwrite = \"overwrite\" in user_message.lower() or \"replace\" in user_message.lower()\n\n    # Determine format preference (default pdf, check message, override for Kaggle)\n    final_format = \"pdf\" # Default format\n    if \"html\" in user_message.lower():\n        final_format = \"html\"\n\n    # Check if running on Kaggle and override to HTML if PDF was intended/defaulted\n    is_kaggle = is_running_on_kaggle()\n    if is_kaggle and final_format == \"pdf\":\n        final_format = \"html\"\n        # Optional: Add a message to history about the format change?\n        # chat_history.append(ChatMessage(role=\"assistant\", content=\"Note: Using HTML format on Kaggle.\"))\n\n    # Determine code inclusion for HTML\n    include_input = not (\"no-input\" in user_message.lower() or \"hide code\" in user_message.lower() or \"without code\" in user_message.lower())\n\n    # --- End Parameter Determination ---\n\n\n    # 5. Extract Path (if provided)\n    # Regex for Windows/Linux paths, including quoted paths\n    path_pattern = r'(?:in|at|from|path|is|=|\\\"|\\')?[ \\t]*([a-zA-Z]:[\\\\/](?:[^\\\"\\'\\\\s\\\\/]| |[\\\\/])+|[/](?:[^\\\"\\'\\\\s\\\\/]| |[\\\\/])+)(?:\\\"|\\'|\\s|$)'\n    path_match = re.search(path_pattern, user_message)\n    provided_path = None\n    if path_match:\n        raw_path = path_match.group(1).strip() # Get the matched path\n        try:\n            provided_path = os.path.normpath(raw_path)\n            print(f\"Detected path: {provided_path}\")\n        except Exception as e:\n            print(f\"Could not normalize detected path '{raw_path}': {e}\")\n            provided_path = raw_path # Use raw path if normalization fails\n\n\n    # 6. --- Handle Conversion Intent ---\n    if conversion_intent:\n\n        # 6a. Check Dependencies before proceeding\n        if not check_and_install_dependencies():\n             chat_history.append(ChatMessage(role=\"assistant\", content=\"Dependency check/installation failed. Cannot proceed with conversion.\"))\n             return chat_history, chat_state\n\n        # 6b. Handle \"convert more files\" request\n        if (\"more\" in user_message.lower() or \"again\" in user_message.lower()):\n            if \"last_directory\" in chat_state[\"context\"]:\n                previous_dir = chat_state[\"context\"][\"last_directory\"]\n                chat_history.append(ChatMessage(role=\"assistant\", content=f\"Converting more notebooks from: {previous_dir} to {final_format.upper()}\"))\n                output_dir = os.path.join(previous_dir, f\"converted_{final_format}\")\n                # Call directory conversion, passing determined format/options\n                results = convert_all_notebooks_in_dir(\n                    previous_dir, output_dir, overwrite, use_safe_copy=True,\n                    format=final_format, include_input=include_input\n                )\n                chat_history.append(ChatMessage(role=\"assistant\", content=\"\\\\n\".join(results)))\n            else:\n                chat_history.append(ChatMessage(role=\"assistant\", content=\"I don't remember a previous directory. Please specify one.\"))\n                chat_state[\"context\"][\"waiting_for\"] = \"directory_path\" # Ask for path\n                # Store current preferences in context for when user provides path\n                chat_state[\"context\"][\"action\"] = \"convert_all\"\n                chat_state[\"context\"][\"overwrite\"] = overwrite\n                chat_state[\"context\"][\"format\"] = final_format\n                chat_state[\"context\"][\"include_input\"] = include_input\n            return chat_history, chat_state\n\n        # 6c. Determine if request is for Directory or Single File\n        is_directory_request = False\n        if (\"all\" in user_message.lower() or \"directory\" in user_message.lower() or \"folder\" in user_message.lower()):\n            is_directory_request = True\n        if provided_path and os.path.isdir(provided_path):\n             # If a valid directory path was provided, treat as directory request\n             is_directory_request = True\n        elif provided_path and provided_path.lower().endswith('.ipynb') and os.path.exists(provided_path):\n             # If a valid file path was provided, treat as single file request\n             is_directory_request = False\n        # Note: Ambiguity remains if no path provided and no keyword used. Defaults to asking.\n\n        # 6d. Handle Directory Conversion Flow\n        if is_directory_request:\n            if provided_path and os.path.isdir(provided_path):\n                # Path provided and valid: Convert directly\n                chat_history.append(ChatMessage(role=\"assistant\",\n                    content=f\"Converting notebooks in directory: {provided_path} to {final_format.upper()}{' (with overwrite)' if overwrite else ''}\"))\n                output_dir = os.path.join(provided_path, f\"converted_{final_format}\")\n                results = convert_all_notebooks_in_dir(\n                    provided_path, output_dir, overwrite, use_safe_copy=True,\n                    format=final_format, include_input=include_input\n                )\n                chat_history.append(ChatMessage(role=\"assistant\", content=\"\\\\n\".join(results)))\n                chat_state[\"context\"][\"last_directory\"] = provided_path # Remember for \"more files\"\n                chat_state[\"context\"][\"waiting_for\"] = None # Clear state\n\n            elif chat_state[\"context\"].get(\"waiting_for\") == \"directory_path\":\n                # User is providing path after being asked\n                path_from_user = os.path.normpath(user_message) # Normalize the input message as path\n                context_overwrite = chat_state[\"context\"].get(\"overwrite\", overwrite) # Use stored or current\n                context_format = chat_state[\"context\"].get(\"format\", final_format)\n                context_include_input = chat_state[\"context\"].get(\"include_input\", include_input)\n\n                if not os.path.isdir(path_from_user):\n                    chat_history.append(ChatMessage(role=\"assistant\", content=f\"Error: '{path_from_user}' is not a valid directory. Please provide a valid directory path.\"))\n                    # Keep waiting_for state active\n                else:\n                    chat_history.append(ChatMessage(role=\"assistant\",\n                        content=f\"Converting notebooks in directory: {path_from_user} to {context_format.upper()}{' (with overwrite)' if context_overwrite else ''}\"))\n                    output_dir = os.path.join(path_from_user, f\"converted_{context_format}\")\n                    results = convert_all_notebooks_in_dir(\n                        path_from_user, output_dir, context_overwrite, use_safe_copy=True,\n                        format=context_format, include_input=context_include_input\n                    )\n                    chat_history.append(ChatMessage(role=\"assistant\", content=\"\\\\n\".join(results)))\n                    chat_state[\"context\"][\"last_directory\"] = path_from_user\n                    # Clear context after successful conversion\n                    chat_state[\"context\"] = {\"last_directory\": path_from_user} # Reset context but keep last_directory\n\n            else:\n                # Ask for directory path\n                chat_history.append(ChatMessage(role=\"assistant\", content=\"Okay, please provide the path to the directory containing the notebooks:\"))\n                chat_state[\"context\"][\"waiting_for\"] = \"directory_path\"\n                chat_state[\"context\"][\"action\"] = \"convert_all\"\n                chat_state[\"context\"][\"overwrite\"] = overwrite\n                chat_state[\"context\"][\"format\"] = final_format\n                chat_state[\"context\"][\"include_input\"] = include_input\n\n        # 6e. Handle Single File Conversion Flow\n        else: # Not a directory request\n            if provided_path and provided_path.lower().endswith('.ipynb') and os.path.exists(provided_path):\n                # Path provided and valid: Convert directly\n                chat_history.append(ChatMessage(role=\"assistant\",\n                    content=f\"Converting notebook: {provided_path} to {final_format.upper()}{' (with overwrite)' if overwrite else ''}\"))\n                output_dir = os.path.join(os.path.dirname(provided_path), f\"converted_{final_format}\")\n\n                if final_format == \"html\":\n                    result = convert_notebook_to_html(provided_path, output_dir, overwrite, use_safe_copy=True, include_input=include_input)\n                else:\n                    result = convert_notebook_to_pdf(provided_path, output_dir, overwrite, use_safe_copy=True)\n\n                chat_history.append(ChatMessage(role=\"assistant\", content=result))\n                chat_state[\"context\"][\"last_directory\"] = os.path.dirname(provided_path) # Remember parent dir\n                chat_state[\"context\"][\"waiting_for\"] = None # Clear state\n\n            elif chat_state[\"context\"].get(\"waiting_for\") == \"file_path\":\n                 # User is providing path after being asked\n                path_from_user = os.path.normpath(user_message) # Normalize the input message as path\n                context_overwrite = chat_state[\"context\"].get(\"overwrite\", overwrite) # Use stored or current\n                context_format = chat_state[\"context\"].get(\"format\", final_format)\n                context_include_input = chat_state[\"context\"].get(\"include_input\", include_input)\n\n                if not os.path.exists(path_from_user):\n                     chat_history.append(ChatMessage(role=\"assistant\", content=f\"Error: File not found at '{path_from_user}'. Please provide a valid path.\"))\n                     # Keep waiting_for state active\n                elif not path_from_user.lower().endswith('.ipynb'):\n                     chat_history.append(ChatMessage(role=\"assistant\", content=f\"Error: '{path_from_user}' is not a Jupyter notebook (.ipynb) file.\"))\n                     # Keep waiting_for state active\n                else:\n                    chat_history.append(ChatMessage(role=\"assistant\",\n                        content=f\"Converting notebook: {path_from_user} to {context_format.upper()}{' (with overwrite)' if context_overwrite else ''}\"))\n                    output_dir = os.path.join(os.path.dirname(path_from_user), f\"converted_{context_format}\")\n\n                    if context_format == \"html\":\n                        result = convert_notebook_to_html(path_from_user, output_dir, context_overwrite, use_safe_copy=True, include_input=context_include_input)\n                    else:\n                        result = convert_notebook_to_pdf(path_from_user, output_dir, context_overwrite, use_safe_copy=True)\n\n                    chat_history.append(ChatMessage(role=\"assistant\", content=result))\n                    chat_state[\"context\"][\"last_directory\"] = os.path.dirname(path_from_user)\n                    # Clear context after successful conversion\n                    chat_state[\"context\"] = {\"last_directory\": os.path.dirname(path_from_user)} # Reset context\n\n            else:\n                 # Ask for file path\n                chat_history.append(ChatMessage(role=\"assistant\", content=\"Okay, please provide the path to the notebook file (.ipynb) you want to convert:\"))\n                chat_state[\"context\"][\"waiting_for\"] = \"file_path\"\n                chat_state[\"context\"][\"action\"] = \"convert_single\"\n                chat_state[\"context\"][\"overwrite\"] = overwrite\n                chat_state[\"context\"][\"format\"] = final_format\n                chat_state[\"context\"][\"include_input\"] = include_input\n\n    # 7. --- Handle General Chat (No Conversion Intent) ---\n    else:\n        try:\n            # Prepare prompt for Gemini\n            prompt = (\n                f\"User message: '{user_message}'\\n\\n\"\n                \"You are a helpful assistant for a notebook-to-PDF/HTML converter tool.\\n\"\n                \"You can help users convert Jupyter notebooks (.ipynb) to PDF or HTML format.\\n\"\n                \"If the user seems to be asking generally about converting notebooks, remind them of the commands:\\n\"\n                \"- 'Convert notebook at [path/to/file.ipynb]' (or '... in ...', '... from ...')\\n\"\n                \"- 'Convert all notebooks in [/path/to/directory]' (or '... folder ...')\\n\"\n                \"- 'Convert more files' (if you previously converted a directory)\\n\"\n                \"- You can specify 'to html' (default on Kaggle) or 'to pdf' (default locally).\\n\"\n                \"- You can add 'without code' or 'no-input' for HTML format.\\n\"\n                \"- You can add 'and overwrite' to replace existing output files.\\n\"\n                \"Keep your response conversational and helpful. If the user asks something unrelated to notebook conversion, politely decline and redirect.\"\n            )\n\n            # Send to Gemini using the chat session stored in state\n            response = chat_state[\"gemini_chat\"].send_message(prompt)\n            chat_history.append(ChatMessage(role=\"assistant\", content=response.text))\n            # Clear any pending action state if Gemini handles the message\n            if \"waiting_for\" in chat_state[\"context\"]:\n                 del chat_state[\"context\"][\"waiting_for\"]\n            if \"action\" in chat_state[\"context\"]:\n                 del chat_state[\"context\"][\"action\"]\n\n\n        except Exception as e:\n            # Handle potential API errors\n            error_message = f\"Sorry, I encountered an error trying to process that: {str(e)}\"\n            chat_history.append(ChatMessage(role=\"assistant\", content=error_message))\n            print(f\"Gemini API Error: {e}\") # Log error to console\n\n    # 8. Return updated history and state\n    return chat_history, chat_state","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d65e89cd","cell_type":"markdown","source":"#### 10. Gradio User Interface (Form Interface Only)\n\nThis section builds the user interface using Gradio (`gr.Blocks`). This version is configured to only display the **form-based interface** for converting notebooks.\n\n* **UI Layout:** Defines the visual components for the form, including:\n    * Radio buttons to select input type ('Single File' or 'Directory').\n    * Conditional display of either a file upload component or a directory path textbox.\n    * Radio buttons for selecting output format (PDF/HTML).\n    * Checkboxes for options like 'Overwrite' and 'Include Code Cells'.\n    * An optional textbox for specifying a base output directory.\n    * A 'Convert' button to trigger the process.\n    * A `gr.Textbox` to display the conversion log output.\n* **Event Handling:** Connects the UI elements to the appropriate Python handler functions defined in previous cells:\n    * The 'Input Type' radio button is linked to `update_visibility` to show/hide the correct input component.\n    * The 'Convert Notebook(s)' button is linked to `handle_conversion_request` to perform the conversion.\n","metadata":{}},{"id":"41297c26","cell_type":"code","source":"# --- GRADIO APP DEFINITION (FORM ONLY) ---\n\nwith gr.Blocks(theme=gr.themes.Default()) as demo: # Using Default theme\n\n    # --- UI Definition ---\n    gr.Markdown(\"# Jupyter Notebook to PDF/HTML Converter\")\n    gr.Markdown(\"## Convert notebooks using the form below\")\n\n    with gr.Row():\n        input_path_type_form = gr.Radio(\n            [\"Single File\", \"Directory\"],\n            label=\"Input Type\",\n            value=\"Single File\"\n        )\n        overwrite_checkbox_form = gr.Checkbox(\n            label=\"Overwrite existing output file(s)\",\n            value=False\n        )\n\n    # Input panels - controlled by radio button\n    with gr.Column(visible=True) as single_file_panel:\n        single_file_input_form = gr.File(\n            label=\"Upload Notebook (.ipynb)\",\n            file_types=[\".ipynb\"],\n            file_count=\"single\"\n        )\n\n    with gr.Column(visible=False) as directory_panel:\n        input_dir_textbox_form = gr.Textbox(\n            label=\"Input Directory Path\",\n            placeholder=\"On Kaggle: Use /kaggle/input/... path after uploading data. Locally: Use local path.\",\n            info=\"Path to the folder containing notebooks. Must be accessible by the environment.\",\n            lines=1\n        )\n\n    # Format selection\n    with gr.Row():\n        format_radio = gr.Radio(\n            [\"PDF\", \"HTML\"],\n            label=\"Preferred Output Format\",\n            value=\"HTML\" if is_running_on_kaggle() else \"PDF\" # Default based on env\n        )\n        include_code_checkbox = gr.Checkbox(\n            label=\"Include Code Cells (HTML only)\",\n            value=True,\n        )\n\n    # Output directory selection\n    with gr.Row():\n        output_dir_textbox_form = gr.Textbox(\n            label=\"Output Base Directory (Optional)\",\n            placeholder=\"Default: /kaggle/working/ or relative to input\",\n            info=\"Specify a base path where the 'converted_...' folder will be created.\"\n        )\n\n    # Action button and log\n    convert_button = gr.Button(\"Convert Notebook(s)\", variant=\"primary\")\n    output_log = gr.Textbox(label=\"Conversion Log\", lines=15, interactive=False, autoscroll=True)\n\n\n    # --- Event Handlers ---\n\n    # Handler for Form Tab Radio button visibility\n    def update_visibility(choice):\n        \"\"\"Updates UI visibility based on input type selection.\"\"\"\n        return {\n            single_file_panel: gr.update(visible=(choice == \"Single File\")),\n            directory_panel: gr.update(visible=(choice == \"Directory\"))\n        }\n\n    input_path_type_form.change(\n        fn=update_visibility,\n        inputs=input_path_type_form,\n        outputs=[single_file_panel, directory_panel]\n    )\n\n    # Handler for Form Tab Convert button\n    # Assumes handle_conversion_request is defined in a previous cell\n    convert_button.click(\n        fn=handle_conversion_request, # Function defined in a previous cell\n        inputs=[\n            input_path_type_form,\n            single_file_input_form,\n            input_dir_textbox_form,\n            output_dir_textbox_form, # Base output dir\n            overwrite_checkbox_form,\n            format_radio,            # Pass format selection\n            include_code_checkbox    # Pass code inclusion preference\n        ],\n        outputs=output_log\n    )\n\n# --- End of Gradio App Definition ---","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"58e4c41b","cell_type":"markdown","source":"#### 11. Launch the app\nLaunch the Gradio app, try both tabs.","metadata":{}},{"id":"3f92027c","cell_type":"code","source":"# Launch the app\n# share=True allows access from outside Kaggle/local network (use with caution)\n# debug=True provides more detailed logs in the console if errors occur\ndemo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"380bad24","cell_type":"code","source":"demo.close()\nprint(\"Gradio app closed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f2de29d1","cell_type":"markdown","source":"#### Project Summary and Next Steps\n\nThis project provides a functional tool for converting Jupyter Notebooks using a Gradio web interface, offering both form-based and chat-based interaction (though the chat relies on Gemini and requires API key setup).\n\n##### Key Functionality Delivered:\n* **Dual Interface:** Users can convert via a structured form or a conversational chat interface.\n* **Form-Based Conversion:** Convert single `.ipynb` files or entire directories using the form.\n* **Chat-Based Conversion:** Use natural language commands to convert files/directories via the chat tab (powered by `process_chat_message` and potentially Gemini).\n* **Format Support:** Converts to **HTML** (especially suitable for Kaggle) or **PDF** (primarily for local use).\n* **Environment Awareness:** Automatically adapts output format (HTML on Kaggle) and output paths (uses `/kaggle/working/`) based on the execution environment.\n* **Robust Conversion:** Uses temporary copies to avoid self-conversion issues and handles common errors.\n* **Dependency Management:** Checks for `nbconvert` and attempts installation.\n* **Secure API Key Handling:** Loads Google API keys securely (needed for Gemini chat and client initialization).\n\n##### Potential Enhancements:\n* Improve error handling and feedback consistency between Form and Chat.\n* Add more specific examples to the Chat interface description.\n* Allow conversion to other formats (e.g., Markdown, Python script).\n* Implement asynchronous conversion for large directories/files.\n\n##### How to Use:\n1.  **Setup:** Ensure your Google API key is configured (via Kaggle Secrets or local environment variables/.env file). This is required for the Gemini chat functionality.\n2.  **Run All Cells:** Execute all cells in the notebook sequentially.\n3.  **Launch:** Run the final `demo.launch()` cell.\n4.  **Use the Form OR Chat:**\n    * **Form Tab:** Select input type, provide file/path, choose options, click 'Convert'.\n    * **Chat Tab:** Type conversion commands (see examples in UI) or ask for help.\n    * **Paths on Kaggle:** Remember to use `/kaggle/input/...` paths for directories after uploading data via '+ Add Data'.\n5.  **Find Output:** Check the 'Conversion Log' (Form) or chat responses for status. Converted files appear in the determined output directory (e.g., `/kaggle/working/..._converted_html` on Kaggle).","metadata":{}},{"id":"40fe254d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}