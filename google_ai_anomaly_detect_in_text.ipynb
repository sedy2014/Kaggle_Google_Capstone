{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Anomaly Detection with Gemini API\n\nThis notebook demonstrates how to detect anomalies in text data using **embeddings** generated by the Gemini API.\n\n## ðŸ’¡ Capabilities of this Notebook\n- Run in **Kaggle or Google Colab** seamlessly\n- Loads and preprocesses newsgroup text data from multiple domains\n- Generates text embeddings using Google's Gemini API\n- Defines a subset of text (**science newsgroups**) as 'normal' data\n- Introduces anomalies by **mixing in unrelated categories in test data**\n- Calculates semantic distances using embeddings\n- **Computes/Finds anomaly text in test data**\n\n\n## ðŸ”§ Possible Enhancements\n- Try different distance metrics (cosine, Mahalanobis)\n- Use alternative embedding models (e.g., BERT, OpenAI)\n- Introduce real-world noisy or domain-shifted data\n- Save and reuse embeddings to avoid repeated API calls","metadata":{}},{"cell_type":"code","source":"# Remove unused conflicting packages\n#!pip uninstall -qqy jupyterlab kfp 2>/dev/null\n# Install specific google-genai version used in the original notebook\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:25:33.148631Z","iopub.execute_input":"2025-04-20T05:25:33.148930Z","iopub.status.idle":"2025-04-20T05:25:36.019569Z","shell.execute_reply.started":"2025-04-20T05:25:33.148909Z","shell.execute_reply":"2025-04-20T05:25:36.018358Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## ðŸ“š Import Necessary Libraries\n\nWe import the required Python modules for:\n- Data loading and preprocessing\n- Using the Gemini API to embed text\n- Distance computation for anomaly scoring\n","metadata":{}},{"cell_type":"code","source":"# Load newsgroup dataset with selected categories\nfrom google import genai\nfrom google.genai import types\nfrom google.api_core import retry\nimport google.api_core.exceptions # Often needed for specific error types\n\nprint(f\"Using google-genai version: {genai.__version__}\")\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf # Still needed for random seed setting initially\nimport time\nimport concurrent.futures\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\nimport email # Standard Python library for parsing email messages\nimport re # Regular expressions for pattern matching and text cleaning\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics.pairwise import cosine_distances # For distance calculation\nfrom sklearn.utils import Bunch # To help manage data sourcing workaround\n\n\n# For demonstration purposes, we use a random seed for reproducibility.\nnp.random.seed(42)\ntf.random.set_seed(42) # Keep for numpy seed consistency if needed elsewhere\n\n# Define the retry predicate function (checks for rate limit or server errors)\n# Ensure this runs before functions using the @retry decorator\nis_retriable = lambda e: isinstance(e, (\n    genai.errors.APIError, # General API errors (includes 503)\n    google.api_core.exceptions.ResourceExhausted, # Specific error for 429\n    google.api_core.exceptions.DeadlineExceeded,\n    google.api_core.exceptions.ServiceUnavailable # Handles 503\n)) and (not hasattr(e, 'code') or e.code in {429, 503}) # Check code if available\n\nprint(\"Imports and retry logic set up.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:25:36.021071Z","iopub.execute_input":"2025-04-20T05:25:36.021354Z","iopub.status.idle":"2025-04-20T05:25:36.029893Z","shell.execute_reply.started":"2025-04-20T05:25:36.021326Z","shell.execute_reply":"2025-04-20T05:25:36.028569Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using google-genai version: 1.7.0\nImports and retry logic set up.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## ðŸ” Set Up Gemini API Key\n\nTo use the Gemini API for generating embeddings, an API key is required.\n\n### Options:\n- **Kaggle**: Secrets are stored under the Kaggle environment and fetched automatically.\n- **Colab**: Use `getpass` or manually input your key. Alternatively, you can use `os.environ`.\n\n> ðŸ’¡ Make sure your API key has access to the embedding endpoint.","metadata":{}},{"cell_type":"code","source":"# Authenticate and create embedding model client\nimport os\n# Make sure genai is imported if not done earlier in Cell 3\n# import google.generativeai as genai\n\n# --- Auto-detect Environment and Get API Key ---\nclient = None # Initialize client to None\nGOOGLE_API_KEY = None\nenvironment = \"unknown\"\n\nprint(\"Attempting to detect environment and configure Google GenAI client...\")\n\n# --- Environment Detection using Environment Variables ---\nif 'COLAB_GPU' in os.environ:\n    print(\"Detected Colab environment via COLAB_GPU.\")\n    environment = \"colab\"\nelif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    print(\"Detected Kaggle environment via KAGGLE_KERNEL_RUN_TYPE.\")\n    environment = \"kaggle\"\nelse:\n    # Fallback check using imports if variables aren't definitive\n    try:\n        from google.colab import userdata\n        print(\"Detected Colab environment via import.\")\n        environment = \"colab\"\n    except ImportError:\n        try:\n            from kaggle_secrets import UserSecretsClient\n            print(\"Detected Kaggle environment via import.\")\n            environment = \"kaggle\"\n        except ImportError:\n             print(\"Could not detect Colab or Kaggle environment via variables or imports.\")\n             environment = \"other\"\n\n\n# --- Get API Key based on detected environment ---\nif environment == \"colab\":\n    try:\n        from google.colab import userdata # Import again just in case\n        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n        print(\"Successfully retrieved GOOGLE_API_KEY from Colab secrets.\")\n    except userdata.SecretNotFoundError:\n        print(\"Secret 'GOOGLE_API_KEY' not found in Colab secrets.\")\n    except Exception as e:\n        print(f\"An error occurred retrieving Colab secret: {type(e).__name__}: {e}\")\n\nelif environment == \"kaggle\":\n    try:\n        from kaggle_secrets import UserSecretsClient # Import again just in case\n        GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n        print(\"Successfully retrieved GOOGLE_API_KEY from Kaggle secrets.\")\n    except Exception as e: # Catch potential errors during secret retrieval\n         print(f\"An error occurred retrieving Kaggle secret: {type(e).__name__}: {e}\")\n         # Check if it's specifically the secret not found error if possible\n         if \"Secret not found\" in str(e): # Simple string check\n              print(\"Secret 'GOOGLE_API_KEY' not found in Kaggle secrets.\")\n         else:\n              print(\"Please ensure the secret 'GOOGLE_API_KEY' is added to this notebook.\")\n\nelif environment == \"other\":\n     # Try environment variable as a last resort\n     print(\"Trying OS environment variable 'GOOGLE_API_KEY'.\")\n     GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n     if GOOGLE_API_KEY:\n          print(\"Found GOOGLE_API_KEY in OS environment variables.\")\n     else:\n          print(\"GOOGLE_API_KEY not found as OS environment variable.\")\nelse: # Should not happen, but handle unknown case\n     print(\"Environment detection resulted in an unexpected state.\")\n\n\n# --- Initialize Client ---\nif GOOGLE_API_KEY:\n    try:\n        # Ensure genai.Client is available\n        if hasattr(genai, 'Client'):\n             client = genai.Client(api_key=GOOGLE_API_KEY)\n             print(\"Successfully configured Google GenAI client.\")\n             # Optional: Test client\n             # try:\n             #      client.models.list()\n             #      print(\"Client connection test successful.\")\n             # except Exception as test_e:\n             #      print(f\"Client connection test failed: {test_e}\")\n             #      client = None # Reset client if test fails\n        else:\n             print(\"Error: genai.Client class not found. Was the library imported correctly?\")\n             client = None\n\n    except Exception as client_e:\n        print(f\"\\n--- ERROR: Failed to configure client ---\")\n        print(f\"An error occurred during client configuration: {type(client_e).__name__}: {client_e}\")\n        client = None\nelse:\n    print(\"\\n--- WARNING ---\")\n    print(\"GOOGLE_API_KEY could not be retrieved.\")\n    print(\"GenAI Client could not be configured.\")\n    print(\"API calls to Gemini will FAIL.\")\n    print(\"--- END WARNING ---\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:25:36.031388Z","iopub.execute_input":"2025-04-20T05:25:36.031724Z","iopub.status.idle":"2025-04-20T05:25:36.303637Z","shell.execute_reply.started":"2025-04-20T05:25:36.031701Z","shell.execute_reply":"2025-04-20T05:25:36.302615Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Attempting to detect environment and configure Google GenAI client...\nDetected Kaggle environment via KAGGLE_KERNEL_RUN_TYPE.\nSuccessfully retrieved GOOGLE_API_KEY from Kaggle secrets.\nSuccessfully configured Google GenAI client.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Dataset\n\n* The [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) is used as the source for our 'normal' data.\n* We load the raw data, preprocess it, and then sample subsets from 'sci.*' categories to define our 'normal' data group.","metadata":{}},{"cell_type":"code","source":"# Load newsgroup dataset with selected categories\n# Load the raw data needed later for finding anomalies\n# Keep these variables accessible\nprint(\"Loading initial train/test splits...\")\n# Use try-except to handle potential network errors during fetch\ntry:\n    newsgroups_train_raw = fetch_20newsgroups(subset=\"train\")\n    newsgroups_test_raw = fetch_20newsgroups(subset=\"test\")\n    print(f\"Raw train posts: {len(newsgroups_train_raw.data)}\")\n    print(f\"Raw test posts: {len(newsgroups_test_raw.data)}\")\n    print(f\"All categories: {newsgroups_train_raw.target_names}\")\nexcept Exception as e:\n    print(f\"ERROR loading dataset: {e}\")\n    print(\"Please ensure internet is enabled for the notebook and try again.\")\n    # Optionally raise the error to stop execution if data loading is critical\n    raise e","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:25:36.312288Z","iopub.execute_input":"2025-04-20T05:25:36.312585Z","iopub.status.idle":"2025-04-20T05:25:36.795758Z","shell.execute_reply.started":"2025-04-20T05:25:36.312563Z","shell.execute_reply":"2025-04-20T05:25:36.794990Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading initial train/test splits...\nRaw train posts: 11314\nRaw test posts: 7532\nAll categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Prepare the Dataset\n\n### **Objective**\n\n* **Preprocessing**: Clean the raw newsgroup posts.\n* **Normalization**: Format the text to resemble standard prose.\n\n### **Processing Steps**\n\n* **Extract**: Use email headers (e.g., \"Subject\") and the message payload.\n* **Remove**: Eliminate email addresses and common headers/footers.\n* **Truncate**: Limit the text length (e.g., 5000 characters).","metadata":{}},{"cell_type":"code","source":"def preprocess_newsgroup_row(data):\n    \"\"\"\n    Processes a single email/newsgroup entry:\n    - Extracts the subject and body.\n    - Removes email addresses and common clutter.\n    - Truncates text to 5,000 characters.\n\n    Args:\n        data (str): Raw email message as a string.\n\n    Returns:\n        str: Cleaned and truncated text.\n    \"\"\"\n    # Parse the email message from the raw string format\n    try:\n        msg = email.message_from_string(data)\n        # Extract subject and body text\n        subject = msg['Subject'] if msg['Subject'] else \"\"\n        payload = msg.get_payload() if msg.get_payload() else \"\"\n        # Ensure payload is a string\n        if isinstance(payload, list): # Handle multipart messages simply\n            # Decode parts if necessary, assuming utf-8 or latin-1\n            payload_parts = []\n            for part in payload:\n                try:\n                    if hasattr(part, 'get_payload'):\n                        p_content = part.get_payload(decode=True)\n                        if p_content:\n                             try:\n                                  payload_parts.append(p_content.decode('utf-8'))\n                             except UnicodeDecodeError:\n                                  try:\n                                       payload_parts.append(p_content.decode('latin-1'))\n                                  except UnicodeDecodeError:\n                                       payload_parts.append(\"[Undecodable Content]\") # Placeholder\n                        else:\n                            payload_parts.append(str(part)) # Fallback for non-payload parts\n                    else:\n                         payload_parts.append(str(part)) # Fallback for non-message parts\n                except Exception as part_e:\n                     payload_parts.append(f\"[Error processing part: {part_e}]\")\n            payload = \"\\n\".join(payload_parts)\n\n        elif isinstance(payload, bytes):\n             try:\n                   payload = payload.decode('utf-8')\n             except UnicodeDecodeError:\n                   try:\n                        payload = payload.decode('latin-1')\n                   except UnicodeDecodeError:\n                        payload = \"[Undecodable Bytes Payload]\"\n\n\n        text = f\"{subject}\\n\\n{str(payload)}\" # Ensure payload is string\n\n    except Exception as e:\n        # Handle potential parsing errors on malformed messages\n        # print(f\"Warning: Error parsing email data: {e}. Using raw data snippet.\")\n        text = str(data)[:5000] # Use raw data as fallback\n\n    # Remove email addresses from the text\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Remove common header/footer lines (simple examples)\n    text = re.sub(r'^\\s*Lines: \\d+\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Organization: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*From: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Subject: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Nntp-Posting-Host: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Article-I\\.D\\.: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*Keywords: .*\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*In article <.*> you write:\\s*$', '', text, flags=re.MULTILINE)\n    text = re.sub(r'wrote:$', '', text, flags=re.MULTILINE) # Common quote intro\n\n\n    # Truncate text to 5,000 characters to limit processing size\n    text = text.strip()[:5000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset, apply_clean):\n    \"\"\"\n    Converts the newsgroup dataset into a structured DataFrame:\n    - Stores text and labels.\n    - Cleans text using preprocess_newsgroup_row() if apply_clean is True.\n    - Maps numeric labels to category names.\n\n    Args:\n        newsgroup_dataset (sklearn.utils.Bunch): Newsgroup dataset object.\n        apply_clean (boolean) : Apply row cleaning\n\n    Returns:\n        pd.DataFrame: Preprocessed dataset with 'Text', 'Label', 'Class Name'.\n    \"\"\"\n    # Convert dataset into a pandas DataFrame\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    if apply_clean:\n        print(f\"Applying text cleaning...\")\n        # Apply text cleaning to each entry\n        df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n        # Remove rows where text became empty after cleaning\n        df = df[df[\"Text\"].str.strip().astype(bool)]\n        print(f\"DataFrame shape after cleaning: {df.shape}\")\n\n\n    # Convert numerical labels to category names\n    target_names = newsgroup_dataset.target_names\n    if target_names:\n         # Ensure label exists in target_names mapping\n         df[\"Class Name\"] = df[\"Label\"].apply(lambda l: target_names[l] if 0 <= l < len(target_names) else \"Unknown\")\n    else:\n         df[\"Class Name\"] = \"Unknown\"\n\n\n    return df.reset_index(drop=True) # Reset index after potential row removal","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:27:59.050969Z","iopub.execute_input":"2025-04-20T05:27:59.051263Z","iopub.status.idle":"2025-04-20T05:27:59.064889Z","shell.execute_reply.started":"2025-04-20T05:27:59.051242Z","shell.execute_reply":"2025-04-20T05:27:59.063559Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"#### create pandas  dataframe from training and test datasets ","metadata":{}},{"cell_type":"code","source":"# Create pandas dataframes from training and test datasets *with* preprocessing\n# Check if raw data was loaded successfully\nif 'newsgroups_train_raw' in locals() and 'newsgroups_test_raw' in locals():\n    print(\"Preprocessing raw train data...\")\n    df_train_full = preprocess_newsgroup_data(newsgroups_train_raw, apply_clean=True)\n    print(\"\\nPreprocessing raw test data...\")\n    df_test_full = preprocess_newsgroup_data(newsgroups_test_raw, apply_clean=True)\n\n    print(\"\\nFull Train DataFrame head:\")\n    print(df_train_full.head())\n    print(f\"\\nFull Train DataFrame shape: {df_train_full.shape}\")\n    print(f\"Full Test DataFrame shape: {df_test_full.shape}\")\n\nelse:\n    print(\"ERROR: Raw data not loaded earlier: Cannot preprocess.\")\n    # Create empty dataframes to avoid subsequent errors, but notebook won't work\n    df_train_full = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])\n    df_test_full = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:28:04.508911Z","iopub.execute_input":"2025-04-20T05:28:04.509209Z","iopub.status.idle":"2025-04-20T05:28:08.909229Z","shell.execute_reply.started":"2025-04-20T05:28:04.509188Z","shell.execute_reply":"2025-04-20T05:28:08.908098Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Preprocessing raw train data...\nApplying text cleaning...\nDataFrame shape after cleaning: (11314, 2)\n\nPreprocessing raw test data...\nApplying text cleaning...\nDataFrame shape after cleaning: (7532, 2)\n\nFull Train DataFrame head:\n                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () \\n...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  \n\nFull Train DataFrame shape: (11314, 3)\nFull Test DataFrame shape: (7532, 3)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"#### Sampling Data to Define \"Normal\" Class ( to differentiate from Anamoly)\n\n*  We sample a subset of the data to represent our \"normal\" dataset for anomaly detection.\n*  Here, we choose the `sci` categories.\n*   We also sample the test set similarly to have a baseline of expected 'normal' test points.","metadata":{}},{"cell_type":"code","source":"# Function to sample data\ndef sample_data(df, num_samples_per_class, classes_to_keep_pattern):\n    \"\"\"\n    Samples rows from the dataset based on the specified number of samples per label\n    and filters the dataset to keep only specified categories matching a pattern. Handles cases where classes have fewer samples than requested.\n\n    Args:\n        df (pd.DataFrame): Input dataframe containing the data ('Text', 'Label', 'Class Name').\n        num_samples_per_class (int): Max number of samples to take per label.\n        classes_to_keep_pattern (str): Substring pattern to filter class names.\n\n    Returns:\n        pd.DataFrame: Filtered and sampled dataframe. Returns empty if no matching classes or input df is empty.\n    \"\"\"\n    if df.empty:\n         # print(\"Debug: Input DataFrame to sample_data is empty.\") # Removed Debug\n         return df\n\n    # Filter rows based on the pattern first\n    df['Class Name'] = df['Class Name'].astype(str)\n    try:\n        filter_mask = df[\"Class Name\"].str.contains(classes_to_keep_pattern, na=False, regex=False)\n        df_filtered = df[filter_mask].copy()\n    except Exception as filter_e:\n        print(f\"Error filtering DataFrame by pattern '{classes_to_keep_pattern}': {filter_e}\")\n        return pd.DataFrame(columns=df.columns) # Return empty on error\n\n\n    if df_filtered.empty:\n        print(f\"Warning: No classes found containing pattern '{classes_to_keep_pattern}' AFTER filtering.\")\n        return df_filtered\n\n    print(f\"Found classes containing '{classes_to_keep_pattern}': {df_filtered['Class Name'].unique().tolist()}\")\n    print(f\"Number of samples before sampling (after filtering): {len(df_filtered)}\")\n\n\n    # Sample rows, selecting num_samples_per_class of each remaining label\n    try:\n        # Ensure the sampling function handles empty groups if any arise\n        df_sampled = (\n            df_filtered.groupby(\"Class Name\", group_keys=False)\n            # Handle potential deprecation warning by explicitly selecting columns if needed,\n            # but standard apply should work. Add random_state for reproducibility.\n            .apply(lambda x: x.sample(min(num_samples_per_class, x.shape[0]), random_state=42) if not x.empty else None)\n        )\n        # Remove potential None results if a group was empty\n        if df_sampled is not None:\n             # Check if df_sampled is a DataFrame before calling dropna\n             if isinstance(df_sampled, pd.DataFrame):\n                  df_sampled.dropna(inplace=True)\n             else: # If apply returned something else (like Series if only one group)\n                  print(f\"Warning: Unexpected result type from groupby.apply: {type(df_sampled)}\")\n                  # Attempt to convert back to DataFrame or handle appropriately\n                  # For simplicity, return empty if structure is unexpected\n                  df_sampled = pd.DataFrame(columns=df_filtered.columns)\n\n        else: # Handle case where apply returns None (e.g., only one empty group)\n             df_sampled = pd.DataFrame(columns=df_filtered.columns)\n\n\n    except Exception as e:\n         print(f\"Error during sampling: {e}\")\n         return pd.DataFrame(columns=df.columns) # Return empty df on error\n\n    print(f\"Number of samples after sampling: {len(df_sampled)}\")\n    return df_sampled.reset_index(drop=True) # Reset index after sampling\n\n# --- Define constants (keep as before) ---\nTRAIN_NUM_SAMPLES_NORMAL = 100\nTEST_NUM_SAMPLES_NORMAL = 25\nNORMAL_CLASSES_PATTERN = \"sci\" # Keep classes containing 'sci' (sci.crypt, sci.electronics, sci.med, sci.space)\n\n# Initialize empty DataFrames for results\ndf_train = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])\ndf_test = pd.DataFrame(columns=['Text', 'Label', 'Class Name'])\n\n# --- Sample 'normal' training data ---\nprint(\"\\n--- Preparing to sample TRAINING data ---\")\nif 'df_train_full' in locals() and not df_train_full.empty:\n    print(f\"\\nSampling 'normal' training data (pattern: {NORMAL_CLASSES_PATTERN})...\")\n    df_train = sample_data(df_train_full, TRAIN_NUM_SAMPLES_NORMAL, NORMAL_CLASSES_PATTERN)\n    if not df_train.empty:\n         print(f\"\\n'Normal' Training samples per class:\\n{df_train['Class Name'].value_counts()}\")\n    else:\n         print(\"Resulting 'normal' training DataFrame (df_train) is empty after sampling.\")\nelse:\n    print(\"Skipping training data sampling as df_train_full is empty or not defined.\")\n\n\n# --- Sample 'normal' test data ---\nprint(\"\\n--- Preparing to sample TEST data ---\")\nif 'df_test_full' in locals() and not df_test_full.empty:\n    print(f\"\\nSampling 'normal' test data (pattern: {NORMAL_CLASSES_PATTERN})...\")\n    df_test = sample_data(df_test_full, TEST_NUM_SAMPLES_NORMAL, NORMAL_CLASSES_PATTERN)\n    if not df_test.empty:\n         print(f\"\\n'Normal' Test samples per class:\\n{df_test['Class Name'].value_counts()}\")\n    else:\n         print(\"Resulting 'normal' test DataFrame (df_test) is empty after sampling.\")\n\nelse:\n     print(\"Skipping test data sampling as df_test_full is empty or not defined.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:28:14.192628Z","iopub.execute_input":"2025-04-20T05:28:14.192895Z","iopub.status.idle":"2025-04-20T05:28:14.230390Z","shell.execute_reply.started":"2025-04-20T05:28:14.192875Z","shell.execute_reply":"2025-04-20T05:28:14.229609Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n--- Preparing to sample TRAINING data ---\n\nSampling 'normal' training data (pattern: sci)...\nFound classes containing 'sci': ['sci.space', 'sci.med', 'sci.electronics', 'sci.crypt']\nNumber of samples before sampling (after filtering): 2373\nNumber of samples after sampling: 400\n\n'Normal' Training samples per class:\nClass Name\nsci.crypt          100\nsci.electronics    100\nsci.med            100\nsci.space          100\nName: count, dtype: int64\n\n--- Preparing to sample TEST data ---\n\nSampling 'normal' test data (pattern: sci)...\nFound classes containing 'sci': ['sci.med', 'sci.space', 'sci.crypt', 'sci.electronics']\nNumber of samples before sampling (after filtering): 1579\nNumber of samples after sampling: 100\n\n'Normal' Test samples per class:\nClass Name\nsci.crypt          25\nsci.electronics    25\nsci.med            25\nsci.space          25\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/745364915.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: x.sample(min(num_samples_per_class, x.shape[0]), random_state=42) if not x.empty else None)\n/tmp/ipykernel_31/745364915.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: x.sample(min(num_samples_per_class, x.shape[0]), random_state=42) if not x.empty else None)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Create the Embeddings\n\nGenerate embeddings for each piece of text using the Gemini API embeddings endpoint. ","metadata":{}},{"cell_type":"markdown","source":"### Task types\n\nThe `text-embedding-004` model supports a `task_type` parameter that generates embeddings tailored for the specific task. For general similarity or anomaly detection based on topic, `RETRIEVAL_DOCUMENT` or `CLUSTERING` might also be suitable, but we'll stick with `CLASSIFICATION` as used previously, assuming it captures general semantic meaning well.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry  # Import retry mechanism for handling API errors\nimport tqdm  # Import tqdm for progress bars\nfrom tqdm.rich import tqdm as tqdmr  # Rich progress bars for better visualization\nimport warnings  # Suppress warnings where necessary\n\n# Add tqdm to Pandas for progress tracking in DataFrame operations\ntqdmr.pandas()\n\n# Suppress experimental warnings from tqdm library\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Define a helper function to retry API calls when quota limits are reached\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable, timeout=300)  # Retry on specific API errors with a timeout of 300 seconds\ndef embed_fn(text: str) -> list[float]:\n    \"\"\"\n    Generates embeddings for a given text using an embedding model.\n\n    Args:\n        text (str): Input text to generate embeddings for.\n\n    Returns:\n        list[float]: Embedding vector as a list of floats.\n    \"\"\"\n    response = client.models.embed_content(\n        model=\"models/text-embedding-004\",  # Specify the embedding model to use\n        contents=text,  # Input text content from DF text column\n        config=types.EmbedContentConfig(\n            task_type=\"classification\",  # Specify task type (e.g., classification)\n        ),\n    )\n\n    return response.embeddings[0].values  # Return the embedding vector\n\ndef create_embeddings(df):\n    \"\"\"\n    Adds an 'Embeddings' column to the DataFrame by generating embeddings from text.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame with a 'Text' column.\n\n    Returns:\n        pd.DataFrame: DataFrame with an additional 'Embeddings' column.\n    \"\"\"\n    df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_fn)  # Apply embedding generation with progress tracking\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:28:18.977261Z","iopub.execute_input":"2025-04-20T05:28:18.977594Z","iopub.status.idle":"2025-04-20T05:28:18.985038Z","shell.execute_reply.started":"2025-04-20T05:28:18.977570Z","shell.execute_reply":"2025-04-20T05:28:18.984237Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"df_train = create_embeddings(df_train)\ndf_test = create_embeddings(df_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:28:21.202758Z","iopub.execute_input":"2025-04-20T05:28:21.203067Z","iopub.status.idle":"2025-04-20T05:34:34.002649Z","shell.execute_reply.started":"2025-04-20T05:28:21.203047Z","shell.execute_reply":"2025-04-20T05:34:34.001284Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fc84be266fd414bbd068d93108eaf3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa19ca0990d648f3b164459d5980c1b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## Anomaly Detection Setup\n\nNow we set up the anomaly detection task:\n1.  **Create Synthetic Anomalies**: Define text examples representing categories completely different from the 'normal' (`sci.*`) newsgroup data, such as financial transactions and spam emails.\n2.  **Embed Anomalies**: Generate embeddings for these synthetic samples using the parallel embedding function.\n3.  **Combine**: Add the synthetic anomalous samples to the 'normal' test set (`df_test`).\n4.  **Calculate Distances**: Compute the distance (e.g., cosine distance) of each point in the combined test set from the centroid (average embedding) of the 'normal' training set (`df_train`).\n5.  **Identify Outliers**: Flag points with distances exceeding a threshold as anomalies. These should ideally be our synthetic examples.","metadata":{}},{"cell_type":"code","source":"# Ensure df_train and df_test are not empty before proceeding\nif df_train.empty or df_test.empty:\n     print(\"ERROR: 'Normal' train or test DataFrame is empty. Cannot proceed with anomaly detection.\")\nelse:\n     # --- 1. Create Synthetic Anomalous Samples ---\n     print(\"Creating synthetic anomaly samples...\")\n\n     # Define synthetic texts for different categories\n     # 5 Financial examples\n     financial_texts = [\n          \"URGENT: Your account statement for March is ready. Payment due April 15th. Click here to view details.\",\n          \"Stock Alert: ACME Corp (ACME) up 5% pre-market trading following positive earnings report.\",\n          \"Transaction Confirmation: $150.75 paid to 'OnlineRetailer'. Your new balance is $1,234.56.\",\n          \"Loan Application Update: Your mortgage pre-approval has been processed. A loan officer will contact you shortly.\",\n          \"Investment Opportunity: Learn about our new high-yield savings account with competitive APY rates.\"\n     ]\n     # 5 Spam examples\n     spam_texts = [\n          \"Congratulations! You've won a FREE iPhone 15! Click HERE to claim your prize NOW!\",\n          \"VIAGRA special offer!! Cheap pills online, discrete shipping guaranteed. Limited time only!\",\n          \"Urgent account security warning! Verify your login details immediately by clicking this link: http://totally-not-a-scam-site.com\",\n          \"Meet hot singles in your area tonight! Join free now, easy registration, instant matches!\",\n          \"Make $1000s working from home! Easy online job opportunity, no experience needed. Apply today!\"\n     ]\n\n     synthetic_texts = financial_texts + spam_texts\n     synthetic_labels = (['Financial'] * 5) + (['Spam'] * 5)\n     num_anomalies_to_sample = len(synthetic_texts) # Should be 10\n\n     # Create DataFrame for anomalies\n     df_anomalies = pd.DataFrame({\n          'Text': synthetic_texts,\n          'Class Name': synthetic_labels,\n          # Add dummy Label if needed by downstream code, though not strictly necessary for detection\n          'Label': [-1] * num_anomalies_to_sample # Assign a dummy label like -1\n     })\n\n     print(f\"Created {num_anomalies_to_sample} synthetic anomaly samples.\")\n     print(f\"Synthetic classes: {df_anomalies['Class Name'].unique().tolist()}\")\n\n\n     # --- 2. Generate Embeddings for Anomalies ---\n     print(\"\\nGenerating embeddings for synthetic anomalies...\")\n     df_anomalies = create_embeddings(df_anomalies) # Uses 'Text' column by default\n\n     # Check if embeddings were generated successfully for anomalies\n     if 'Embeddings' not in df_anomalies.columns or df_anomalies.empty:\n          print(\"Error: Failed to generate embeddings for synthetic anomaly samples. Cannot proceed.\")\n          # Handle error\n     else:\n          print(f\"Successfully generated embeddings for {len(df_anomalies)} synthetic anomaly samples.\")\n\n          # --- 3. Combine Anomalies with Test Set ---\n          # Add a flag to distinguish anomalies\n          df_anomalies['Is_Anomaly'] = True\n          # Add flag to original test set (ensure it hasn't been added before)\n          if 'Is_Anomaly' not in df_test.columns:\n               df_test['Is_Anomaly'] = False\n          else: # Ensure existing flags are False for the normal test set\n              df_test['Is_Anomaly'] = False\n\n\n          # Combine the original test set with the new anomalies\n          df_test_combined = pd.concat([df_test, df_anomalies], ignore_index=True)\n          print(f\"\\nCombined test set size: {len(df_test_combined)} rows\")\n\n          # --- 4. Detect Anomalies using Distance from Training Centroid ---\n          print(\"Calculating training centroid and distances...\")\n\n          # Ensure training embeddings are ready\n          if 'Embeddings' not in df_train.columns or df_train.empty:\n               print(\"Error: Training embeddings not available. Cannot calculate centroid.\")\n          else:\n                x_train_embeddings = np.stack(df_train['Embeddings'].values)\n                # Ensure combined test embeddings are ready (check after concat might be needed if anomalies failed)\n                if 'Embeddings' not in df_test_combined.columns or df_test_combined['Embeddings'].isnull().any():\n                     print(\"Warning: Missing embeddings in combined test set after concat. Dropping rows with missing embeddings.\")\n                     df_test_combined.dropna(subset=['Embeddings'], inplace=True)\n                     print(f\"Proceeding with {len(df_test_combined)} rows in combined test set.\")\n\n\n                if not df_test_combined.empty: # Check if still have data after potential drop\n                     x_test_combined_embeddings = np.stack(df_test_combined['Embeddings'].values)\n\n                     # Calculate the centroid (mean embedding) of the 'normal' training data\n                     train_centroid = np.mean(x_train_embeddings, axis=0)\n\n                     # Calculate cosine distance from each point in the combined test set to the training centroid\n                     distances = cosine_distances(x_test_combined_embeddings, train_centroid.reshape(1, -1))\n                     df_test_combined['Distance_to_Centroid'] = distances.flatten()\n\n                     # --- 5. Identify & Save Anomalies ---\n                     # Determine a threshold (e.g., 90th percentile of distances)\n                     valid_distances = df_test_combined['Distance_to_Centroid'].dropna()\n                     if not valid_distances.empty:\n                          # Calculate percentile based on expected number of anomalies\n                          # num_total = len(df_test_combined)\n                          # anomaly_percentile = (1 - (num_anomalies_to_sample / num_total)) * 100 if num_total > 0 else 90\n                          # Using fixed 90th percentile for simplicity\n                          anomaly_percentile = 90\n                          distance_threshold = np.percentile(valid_distances, anomaly_percentile)\n                          print(f\"Using distance threshold ({anomaly_percentile:.0f}th percentile): {distance_threshold:.4f}\")\n\n                          # Flag potential anomalies based on the threshold\n                          df_test_combined['Detected_Anomaly'] = df_test_combined['Distance_to_Centroid'] > distance_threshold\n\n                          # Separate the detected anomalies\n                          detected_anomalies_df = df_test_combined[df_test_combined['Detected_Anomaly'] == True].copy() # Explicit check for True\n\n                          print(\"\\n--- Detected Anomalies ---\")\n                          if not detected_anomalies_df.empty:\n                               # Display info about detected anomalies\n                               print(detected_anomalies_df[['Text', 'Class Name', 'Is_Anomaly', 'Distance_to_Centroid']].head())\n\n                               # Compare actual injected vs detected\n                               # Recalculate num_anomalies_present in the potentially filtered df_test_combined\n                               num_injected_present = df_test_combined['Is_Anomaly'].sum()\n                               correctly_detected = detected_anomalies_df['Is_Anomaly'].sum()\n                               false_positives = len(detected_anomalies_df) - correctly_detected\n                               print(f\"\\nTotal detected: {len(detected_anomalies_df)}\")\n                               print(f\"Correctly identified injected anomalies: {correctly_detected}/{num_injected_present} (Expected {num_anomalies_to_sample} injected initially)\")\n                               print(f\"Incorrectly identified (false positives): {false_positives}\")\n\n                               # Save the detected anomalies to a CSV\n                               try:\n                                    anomaly_filename = 'detected_synthetic_anomalies.csv'\n                                    # Select columns to save\n                                    columns_to_save = ['Text', 'Class Name', 'Is_Anomaly', 'Distance_to_Centroid', 'Detected_Anomaly', 'Embeddings']\n                                    detected_anomalies_df[columns_to_save].to_csv(anomaly_filename, index=False)\n                                    print(f\"\\nDetected anomalies saved to {anomaly_filename}\")\n                               except Exception as e:\n                                    print(f\"\\nError saving anomalies to CSV: {e}\")\n                          else:\n                               print(\"No anomalies detected above the threshold.\")\n                     else:\n                          print(\"Error: No valid distances found to calculate threshold.\")\n\n                     # Display the head of the combined test df showing new columns\n                     print(\"\\n--- Head of Combined Test Set with Anomaly Info ---\")\n                     print(df_test_combined[['Text', 'Class Name', 'Is_Anomaly', 'Distance_to_Centroid', 'Detected_Anomaly']].head())\n                else:\n                    print(\"Combined test set is empty after handling missing embeddings.\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T05:48:19.635085Z","iopub.execute_input":"2025-04-20T05:48:19.635331Z","iopub.status.idle":"2025-04-20T05:48:21.886393Z","shell.execute_reply.started":"2025-04-20T05:48:19.635317Z","shell.execute_reply":"2025-04-20T05:48:21.885503Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660710cc766449cf8234cdfb9c4715cb"}},"metadata":{}},{"name":"stdout","text":"Creating synthetic anomaly samples...\nCreated 10 synthetic anomaly samples.\nSynthetic classes: ['Financial', 'Spam']\n\nGenerating embeddings for synthetic anomalies...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"Successfully generated embeddings for 10 synthetic anomaly samples.\n\nCombined test set size: 110 rows\nCalculating training centroid and distances...\nUsing distance threshold (90th percentile): 0.1956\n\n--- Detected Anomalies ---\n                                                  Text Class Name  Is_Anomaly  \\\n51   cure for dry skin?\\n\\nHi all,\\n\\nMy skin is ve...    sci.med       False   \n100  URGENT: Your account statement for March is re...  Financial        True   \n101  Stock Alert: ACME Corp (ACME) up 5% pre-market...  Financial        True   \n102  Transaction Confirmation: $150.75 paid to 'Onl...  Financial        True   \n103  Loan Application Update: Your mortgage pre-app...  Financial        True   \n\n     Distance_to_Centroid  \n51               0.225113  \n100              0.323991  \n101              0.235453  \n102              0.349619  \n103              0.269010  \n\nTotal detected: 11\nCorrectly identified injected anomalies: 10/10 (Expected 10 injected initially)\nIncorrectly identified (false positives): 1\n\nDetected anomalies saved to detected_synthetic_anomalies.csv\n\n--- Head of Combined Test Set with Anomaly Info ---\n                                                Text Class Name  Is_Anomaly  \\\n0  Re: The Escrow Database.\\n\\nIn article <>  (Da...  sci.crypt       False   \n1  Re: New Encryption Algorithm\\n\\n (Arthur Melni...  sci.crypt       False   \n2  Re: New Encryption Algorithm\\n\\n\\nArthur Melni...  sci.crypt       False   \n3  Re: Clipper considered harmful\\n\\n>Sternlight)...  sci.crypt       False   \n4  Re: Key Registering Bodies\\n\\nIn article <>, \\...  sci.crypt       False   \n\n   Distance_to_Centroid  Detected_Anomaly  \n0              0.090971             False  \n1              0.034728             False  \n2              0.043924             False  \n3              0.051283             False  \n4              0.023261             False  \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}